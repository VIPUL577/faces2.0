{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from FPN import Features, FPNetwork , classificationhead , bboxhead\n",
    "from Loss import Lossfunction\n",
    "from datasets import load_dataset\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from torch.cuda.amp import  GradScaler\n",
    "from torch.amp import autocast\n",
    "import gc\n",
    "from dataset_convert import AnchorGenerator, FaceDetectionDataset\n",
    "# device = torch.device(\"mps\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/vipulagarwal/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "# model = model.features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"CUHK-CSE/wider_face\")\n",
    "train_dataset = dataset['train'].with_format(\"torch\")\n",
    "val_dataset = dataset['validation'].with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Features(model,['3','6', '13','18'])\n",
    "topdown = FPNetwork(out_channels=256)\n",
    "classifier = classificationhead(channels=256, num_anchors= 12, num_of_classes= 1)\n",
    "bboxregression = bboxhead(channels= 256 , num_anchors= 12)\n",
    "loss =Lossfunction(lambd=10)\n",
    "anchors = AnchorGenerator()\n",
    "data = FaceDetectionDataset(train_dataset,anchors)\n",
    "vali_data = FaceDetectionDataset(val_dataset,anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /Users/vipulagarwal/Documents/AI & ML/Projects/New_FR/version2/faces2.0/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/3dvbqf1x7ts37h_0ccg5yh5r0000gn/T/ipykernel_12899/1582781517.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd_extractor = torch.load('/Users/vipulagarwal/Documents/AI & ML/Projects/New_FR/version2/faces2.0/models/extractor.pt',map_location=torch.device('cpu'))\n",
      "/var/folders/pc/3dvbqf1x7ts37h_0ccg5yh5r0000gn/T/ipykernel_12899/1582781517.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd_classifier = torch.load('/Users/vipulagarwal/Documents/AI & ML/Projects/New_FR/version2/faces2.0/models/classifier.pt',map_location=torch.device('cpu'))\n",
      "/var/folders/pc/3dvbqf1x7ts37h_0ccg5yh5r0000gn/T/ipykernel_12899/1582781517.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd_bboxregression = torch.load('/Users/vipulagarwal/Documents/AI & ML/Projects/New_FR/version2/faces2.0/models/bboxregression.pt',map_location=torch.device('cpu'))\n",
      "/var/folders/pc/3dvbqf1x7ts37h_0ccg5yh5r0000gn/T/ipykernel_12899/1582781517.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd_topdown = torch.load('/Users/vipulagarwal/Documents/AI & ML/Projects/New_FR/version2/faces2.0/models/topdown.pt',map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_extractor = torch.load('/Users/vipulagarwal/Documents/AI & ML/Projects/New_FR/version2/faces2.0/models/extractor.pt',map_location=torch.device('cpu'))\n",
    "sd_classifier = torch.load('/Users/vipulagarwal/Documents/AI & ML/Projects/New_FR/version2/faces2.0/models/classifier.pt',map_location=torch.device('cpu'))\n",
    "sd_bboxregression = torch.load('/Users/vipulagarwal/Documents/AI & ML/Projects/New_FR/version2/faces2.0/models/bboxregression.pt',map_location=torch.device('cpu'))\n",
    "sd_topdown = torch.load('/Users/vipulagarwal/Documents/AI & ML/Projects/New_FR/version2/faces2.0/models/topdown.pt',map_location=torch.device('cpu'))\n",
    "\n",
    "extractor.load_state_dict(sd_extractor)\n",
    "topdown.load_state_dict(sd_topdown)\n",
    "classifier.load_state_dict(sd_classifier)\n",
    "bboxregression.load_state_dict(sd_bboxregression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(p):\n",
    "    features = extractor.extract(p)\n",
    "    newfeatures = topdown(features)\n",
    "    output = {}\n",
    "    for key in list(newfeatures.keys()):\n",
    "        temp = {}\n",
    "        temp[\"bbox\"] = bboxregression(newfeatures[key])\n",
    "        temp[\"cls\"] = classifier(newfeatures[key])\n",
    "        output[key] = temp\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forward:\n",
    "    def __init__(self, extractor, topdown, bboxregression, classifier):\n",
    "        self.extractor=extractor\n",
    "        # self.extractor = Features(extractor,['3','6', '13','18'])\n",
    "        self.topdown = topdown\n",
    "        self.bboxregression = bboxregression\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.raw_features = self.extractor.extract(inp)\n",
    "        self.newfeatures = self.topdown(self.raw_features)\n",
    "        output = {}\n",
    "        for key in list(self.newfeatures.keys()):\n",
    "            temp = {}\n",
    "            temp[\"bbox\"] = self.bboxregression(self.newfeatures[key])\n",
    "            temp[\"cls\"] = self.classifier(self.newfeatures[key])\n",
    "            output[key] = temp\n",
    "            self.output = output\n",
    "        return self.output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd= Forward(extractor=extractor,\n",
    "            topdown=topdown,\n",
    "            bboxregression=bboxregression,\n",
    "            classifier=classifier\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp= data[200][0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Lossfunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = fd(inp)\n",
    "kk=data[200][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4800, 1])\n",
      "tensor(0)\n",
      "torch.Size([19200, 1])\n",
      "tensor(12)\n",
      "torch.Size([76800, 1])\n",
      "tensor(69)\n",
      "torch.Size([307200, 1])\n",
      "tensor(178)\n"
     ]
    }
   ],
   "source": [
    "for key in range(0,4):\n",
    "    print(kk[key]['cls_targets'].shape)\n",
    "    print(kk[key]['cls_targets'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5621)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = nn.Sequential(\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    )\n",
    "ak = transform(train_dataset[100]['image']/255)\n",
    "ak[0].mean() #0.0549\n",
    "ak[1].mean() #0.0022\n",
    "ak[2].mean() #-0.5621\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak = transform(train_dataset[89]['image']/255).unsqueeze(0)\n",
    "oak =model(ak)\n",
    "ooak= torch.softmax(oak,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6657e+00, -1.0125e+00, -1.1825e+00, -2.6803e+00, -1.6418e+00,\n",
       "         -2.7974e-01, -9.7216e-01,  1.2186e+00,  8.8198e-01, -1.2936e+00,\n",
       "         -1.6477e+00, -1.4410e+00, -3.5058e-01, -1.5865e+00, -1.4154e+00,\n",
       "         -1.0959e+00, -1.0114e+00, -2.5430e-01, -1.1257e+00, -8.2800e-01,\n",
       "         -1.5392e+00, -6.1431e-01, -1.8899e+00,  4.7935e-01, -1.2840e+00,\n",
       "         -1.6750e+00, -1.6561e+00, -1.9692e+00, -7.7283e-01, -9.5115e-01,\n",
       "         -5.7824e-01, -1.2913e+00, -6.7403e-01, -6.7691e-01,  4.0566e-01,\n",
       "         -3.1791e-01,  6.1518e-01, -6.2576e-01, -9.2850e-01,  2.6857e-01,\n",
       "         -4.4177e-01, -5.1533e-01, -1.1532e+00, -4.2500e-03, -9.7136e-01,\n",
       "         -1.1019e+00, -1.8175e-01, -2.2373e-01, -2.1664e+00, -1.6844e+00,\n",
       "         -1.2037e+00,  7.7944e-01, -9.3508e-01, -1.0522e+00, -7.4613e-01,\n",
       "         -1.5496e+00, -6.7422e-01, -1.1558e+00, -9.1632e-01, -2.8629e-01,\n",
       "          2.2499e-01,  8.6036e-02,  1.9929e-01, -7.2600e-01, -6.4295e-01,\n",
       "         -1.0848e+00, -6.0790e-01, -8.7318e-01, -1.4036e+00, -1.2755e+00,\n",
       "         -2.0763e+00, -5.7259e-02, -1.6125e+00, -6.4876e-02, -1.4694e+00,\n",
       "         -8.2187e-01,  3.5821e-01, -4.1232e-01,  4.0411e-01, -1.7467e-01,\n",
       "         -4.4312e-01, -1.9834e+00,  7.4592e-02, -4.9144e-01, -6.0066e-01,\n",
       "         -1.4509e-01,  3.0703e-01,  5.3851e-01,  5.5408e-01,  4.3588e-03,\n",
       "         -1.7882e+00, -5.5147e-01, -2.1625e+00, -7.3981e-01,  1.5351e-02,\n",
       "         -2.8265e+00, -3.0964e-01, -3.5368e-01, -2.4143e+00, -9.6250e-02,\n",
       "         -1.9062e+00, -7.3951e-01, -1.5852e+00, -3.3528e-01, -8.8509e-01,\n",
       "         -9.2545e-01, -8.4984e-01, -1.1467e+00, -1.8090e+00, -2.1585e+00,\n",
       "         -1.8767e+00, -2.2863e+00,  6.7592e-01, -8.7426e-01,  7.2837e-02,\n",
       "         -1.6990e+00, -1.7729e+00, -1.5843e+00,  9.4099e-01,  3.5393e-01,\n",
       "         -9.3045e-01,  6.7581e-01,  7.9387e-01, -3.5554e-01,  1.3359e+00,\n",
       "          1.4451e-01, -5.4072e-01, -2.1592e+00, -1.4648e+00, -1.2786e+00,\n",
       "         -2.8667e+00, -1.8438e+00, -8.0380e-01, -1.6643e+00, -8.8049e-01,\n",
       "         -2.7549e+00, -3.2456e-01, -1.4188e+00, -1.8281e+00, -2.1938e+00,\n",
       "         -1.6885e+00, -2.3044e+00, -2.5678e+00, -2.1070e+00, -1.5413e+00,\n",
       "         -9.8839e-01, -1.4730e+00, -2.4417e+00, -1.4586e+00, -2.3644e+00,\n",
       "          2.3779e-01,  2.6503e+00, -9.1298e-01,  1.0904e-01,  5.5238e-02,\n",
       "          7.5100e-01, -6.2254e-03,  2.7311e-01,  3.2638e-01,  6.6983e-01,\n",
       "          1.6163e-01,  6.6928e-01,  1.7074e-01,  7.9781e-01,  8.0973e-02,\n",
       "          4.9438e-02,  2.3862e-01,  8.4155e-02,  1.2964e+00,  3.4872e-01,\n",
       "          1.6414e+00,  1.5046e+00,  1.2562e+00,  6.5388e-01,  8.3210e-01,\n",
       "         -9.0772e-02,  1.2288e+00,  7.7624e-01,  1.3576e+00,  5.9073e-01,\n",
       "          1.0248e+00,  2.8314e-01,  1.0897e+00, -6.1956e-01,  9.3198e-01,\n",
       "          9.8705e-01,  1.1878e+00, -4.4903e-01,  2.2510e-01,  2.9549e-01,\n",
       "         -4.9566e-01, -4.7292e-01,  1.2870e+00,  2.3272e-01, -1.9311e-01,\n",
       "          1.2872e+00, -1.7481e-02,  6.1870e-01, -9.4156e-02,  6.4761e-01,\n",
       "          2.7326e-01, -2.8782e-01,  4.5633e-01,  1.5942e+00,  1.0975e+00,\n",
       "          9.0236e-01,  6.7252e-01,  1.8463e+00,  2.3273e+00,  1.0808e+00,\n",
       "         -2.4965e-02,  5.0522e-01,  1.3179e+00,  9.3314e-01,  4.4495e-01,\n",
       "          1.1180e+00,  5.5198e-01,  9.0036e-01,  7.2974e-01,  1.0504e+00,\n",
       "          3.5920e-01,  4.9191e-02,  1.5028e+00,  1.8649e-01, -9.1376e-03,\n",
       "          7.9621e-01,  2.6315e-01,  1.2102e+00,  2.7136e-01,  4.1489e-01,\n",
       "         -4.7934e-01,  9.9585e-01,  9.6218e-01,  2.3129e-01,  6.8263e-01,\n",
       "          6.9294e-01,  3.7384e-01,  5.2879e-01,  1.0190e+00, -3.7319e-01,\n",
       "          1.1990e-02,  5.5681e-02,  2.9275e-01,  7.0316e-01,  5.3681e-02,\n",
       "          5.6528e-01, -1.8067e-01,  4.5217e-01,  7.9634e-01,  3.7275e-01,\n",
       "          6.3310e-01,  9.9878e-01, -8.4618e-01,  1.0254e+00,  1.4736e+00,\n",
       "         -5.0788e-01,  4.0740e-01,  2.3400e+00,  7.8344e-01,  2.8505e-01,\n",
       "          1.3295e-01, -6.0930e-01, -1.1658e-01,  5.5587e-01,  1.2266e+00,\n",
       "          1.0780e+00,  4.8312e-01,  1.0336e+00,  5.6522e-01, -2.5472e-01,\n",
       "         -2.7540e-01, -1.0058e+00, -1.1508e+00,  7.5191e-01, -1.5139e+00,\n",
       "         -1.4980e+00, -2.3411e-01, -1.4869e+00, -1.9321e+00, -1.9438e-01,\n",
       "         -9.5934e-01,  9.6824e-01,  1.0063e+00, -9.9623e-02,  9.4294e-01,\n",
       "          1.0048e+00,  3.9666e-01, -1.7825e-02, -6.2595e-02, -1.5036e+00,\n",
       "         -1.1902e+00, -9.6660e-01, -3.8123e-01, -7.2592e-01, -8.6404e-01,\n",
       "         -1.4685e+00, -6.9565e-01, -2.4124e+00, -1.0482e+00, -7.1885e-01,\n",
       "         -1.0674e+00, -1.1760e+00, -4.4851e-01, -2.0950e-01, -1.4489e+00,\n",
       "         -1.1937e+00,  6.0521e-01, -1.0957e+00, -1.4826e+00, -5.4548e-01,\n",
       "          4.8814e-01, -5.0188e-01, -3.8831e-01,  1.8223e-01,  9.5856e-01,\n",
       "         -4.3033e-01, -4.0078e-01, -1.6013e+00, -1.6231e+00,  2.7240e-02,\n",
       "         -1.7083e+00, -1.9123e+00, -1.8519e+00, -2.0612e+00, -2.0923e+00,\n",
       "         -2.5440e+00, -1.2325e+00, -6.6945e-01, -8.4314e-01, -1.1267e+00,\n",
       "         -7.8391e-01, -5.3437e-01, -4.5708e-01, -3.1063e-02, -1.5656e-01,\n",
       "         -1.3065e+00, -1.6092e+00,  3.9077e-01,  1.8773e-01, -7.0959e-01,\n",
       "         -7.6531e-02,  8.6200e-01, -3.8306e-01, -6.6489e-02, -9.6289e-01,\n",
       "          1.0899e+00,  1.8929e-01, -1.9703e+00, -1.8625e-01, -1.2419e+00,\n",
       "         -9.6722e-01, -1.6190e+00, -1.0948e+00, -7.6929e-01,  5.4587e-01,\n",
       "          1.6781e+00,  1.5439e+00,  2.9194e-01,  4.8478e-01,  2.7108e-01,\n",
       "          3.3898e-01, -2.1920e-01, -3.6215e-01, -3.7767e-01, -5.2590e-01,\n",
       "         -8.3626e-01, -2.3488e+00, -1.0295e+00, -1.7389e+00, -1.6193e+00,\n",
       "         -9.2658e-01, -4.0712e-01, -7.5824e-01,  5.6664e-01, -7.3063e-01,\n",
       "         -1.0289e+00, -1.4368e+00, -1.3953e-01, -1.6006e-01, -2.4489e-01,\n",
       "         -3.1850e-01, -1.2983e+00, -1.0063e+00, -1.1241e+00, -1.0149e+00,\n",
       "         -1.9125e-01, -3.7976e-01, -1.6983e+00, -1.3891e+00, -5.7593e-01,\n",
       "         -4.9063e-01, -2.9380e-01, -1.5036e+00, -2.5945e+00, -6.5542e-01,\n",
       "         -5.2892e-01, -1.7228e+00, -5.0048e-01,  1.9074e-01,  5.8921e-01,\n",
       "          1.4355e-01,  1.2960e+00,  6.0109e-02, -3.9135e+00, -1.5446e+00,\n",
       "         -7.9833e-01,  2.0579e-01, -4.7731e-01, -2.7276e-01,  9.7614e-01,\n",
       "         -7.0068e-01,  6.4007e-03,  1.8010e+00,  9.7366e-01,  7.4294e-01,\n",
       "          9.6609e-01, -2.2274e-01,  7.2811e-02, -2.8587e-01,  9.0330e-01,\n",
       "          4.2161e-01,  9.6030e-01, -5.1800e-01,  3.6967e-01,  1.7379e-01,\n",
       "         -3.7108e-01, -8.5108e-04,  1.5894e+00,  2.3022e+00,  6.3300e-01,\n",
       "         -1.5271e-01, -2.7803e-01,  7.6330e-01,  6.4617e-01,  7.2348e-01,\n",
       "          1.4712e+00, -6.7529e-01,  4.0159e-01, -6.1342e-02, -6.6306e-01,\n",
       "          1.0421e+00,  2.9994e-01, -5.8594e-03, -5.4652e-01, -1.9550e-01,\n",
       "          2.0151e-01,  1.1127e-01,  2.6243e+00,  1.1584e+00, -2.2566e-01,\n",
       "         -7.7145e-01,  1.1495e+00,  1.1654e+00, -2.3357e-02, -2.1293e-02,\n",
       "          1.0818e+00,  1.6317e+00,  1.7008e+00,  3.9761e-02,  1.0024e+00,\n",
       "         -1.0611e+00,  1.4124e+00,  1.0356e+00,  2.4834e+00,  1.7178e+00,\n",
       "         -3.0496e-01, -1.5343e+00, -2.9752e-01, -5.5059e-01,  1.7263e+00,\n",
       "          1.6797e+00, -5.9008e-02,  5.6944e-02,  5.0361e-01, -7.7688e-01,\n",
       "         -3.7116e-02, -2.7012e-01,  7.2571e-01,  1.1874e+00,  2.2649e-01,\n",
       "         -1.7410e-01,  1.8478e-01,  6.8901e-01, -7.2414e-01, -1.2731e+00,\n",
       "          1.9055e-01, -7.7918e-01,  1.0086e+00,  1.7412e+00,  9.0103e-01,\n",
       "          9.9003e-02,  5.3059e-01,  2.1447e-01, -6.8915e-01,  7.1655e-01,\n",
       "         -5.7543e-01, -1.0673e+00,  1.4680e-02, -2.5030e-01,  1.3020e+00,\n",
       "         -1.5301e+00,  1.9187e+00,  9.2952e-01,  5.9410e-01,  1.3915e+00,\n",
       "          1.3477e+00, -4.0212e-02,  1.9323e-01, -5.4528e-02, -1.1357e-01,\n",
       "         -1.5716e+00, -9.4122e-01,  5.8514e-02,  6.7934e-01,  1.5586e+00,\n",
       "          2.5819e+00,  6.5773e-01, -3.5295e-01,  1.5554e+00,  5.1436e-01,\n",
       "         -9.4292e-01, -8.0974e-02,  1.0604e+00,  1.6473e+00,  4.4864e-01,\n",
       "         -1.0303e+00,  1.4613e-01,  2.7507e-02,  4.6364e-01,  2.5416e-01,\n",
       "          1.2283e+00,  2.1911e-01,  2.2825e-01, -1.2505e+00,  3.6542e-01,\n",
       "         -9.1152e-01, -4.2049e-01, -2.3367e-01,  4.3691e-01,  1.6990e+00,\n",
       "         -2.5689e+00,  1.8720e+00,  1.7104e+00,  2.9144e-01,  2.5186e-01,\n",
       "          1.5654e+00, -2.1837e-01, -1.9456e+00, -1.5922e+00, -1.4752e-01,\n",
       "          2.6736e-01,  4.2452e-01,  1.5682e+00,  3.4035e-01, -1.7368e+00,\n",
       "         -7.0051e-01,  9.8525e-01,  1.5162e-01,  1.0868e+00,  1.2051e+00,\n",
       "          7.4919e-01, -5.4302e-01,  4.3722e-01, -5.3721e-01, -7.0016e-01,\n",
       "         -1.0190e+00,  1.5363e-01,  4.5903e-01,  1.4747e-01, -5.0529e-01,\n",
       "          2.3282e+00, -6.4071e-02,  1.3293e+00, -1.7332e+00,  8.8109e-03,\n",
       "         -7.5318e-01, -1.7504e+00,  1.5687e+00,  6.7084e-01, -3.4331e-01,\n",
       "          1.8154e-01, -4.1876e-01,  3.7214e-01,  1.0311e+00,  1.3301e-01,\n",
       "          1.2179e+00, -2.5783e-01,  1.2619e+00,  1.2753e+00,  1.2226e+00,\n",
       "         -1.1462e+00,  1.0848e+00, -1.0026e+00,  1.0759e+00, -1.0205e-01,\n",
       "         -7.0009e-01,  1.3667e+00,  1.9874e-01, -8.1879e-01,  1.0679e+00,\n",
       "          2.5454e+00,  2.8413e-01,  1.5869e-01,  4.8318e-02,  4.7221e-01,\n",
       "         -1.0501e-01,  1.3285e+00, -1.3172e+00,  4.7264e-01, -6.4235e-01,\n",
       "          9.4891e-01,  2.0190e-01, -7.3815e-01,  1.1872e+00,  1.3902e-01,\n",
       "          5.5949e-01,  1.4394e+00,  7.1112e-01,  2.2489e+00,  1.2360e+00,\n",
       "          9.2231e-01,  3.4093e-01, -6.1154e-02,  8.0805e-01,  1.5815e-01,\n",
       "         -1.4318e+00,  1.6907e+00, -3.6836e-01, -9.4119e-01, -6.9695e-02,\n",
       "         -1.4205e-01,  8.9316e-01,  1.4015e+00,  1.7366e+00, -5.8358e-02,\n",
       "          2.3617e-01,  1.1260e+00,  1.8840e-01,  9.5621e-01,  1.3565e-01,\n",
       "         -1.5941e+00,  1.2696e+00,  1.2912e-01,  2.7915e+00,  2.9259e-01,\n",
       "         -8.2820e-01,  6.7162e-01,  2.9049e-01, -5.3525e-01, -1.2859e+00,\n",
       "          1.6265e+00,  1.4428e-01,  3.0737e-01,  3.0441e-01, -4.5386e-01,\n",
       "          6.5243e-01,  3.1646e-01, -3.4949e-01,  1.5022e-01, -1.1726e-01,\n",
       "          4.6287e-01, -6.1340e-01, -1.2319e-01, -2.7823e-01,  6.8574e-01,\n",
       "         -7.4101e-01,  1.1294e+00,  1.8961e+00, -7.7458e-01, -3.6690e-01,\n",
       "         -6.4475e-01,  1.9336e-01,  4.8947e-01,  2.1181e-01,  1.2934e+00,\n",
       "         -6.6158e-01,  2.0629e+00,  1.8977e+00,  1.3585e+00, -2.5212e-01,\n",
       "          1.2125e-01,  2.2610e-01,  2.2004e-01,  3.3946e-01,  6.0069e-01,\n",
       "         -2.4375e+00, -1.7566e-01, -1.0392e+00, -5.6202e-01, -7.1320e-01,\n",
       "         -5.8059e-01,  1.6835e+00,  7.2520e-01,  6.1536e-01, -1.6245e+00,\n",
       "          1.0563e+00,  1.4769e+00, -9.7084e-01, -3.2675e-01,  1.4662e+00,\n",
       "          2.2253e+00,  1.4257e-01,  3.4139e-01,  2.3796e-01,  9.8249e-01,\n",
       "          5.1481e-01,  1.5415e-02,  4.5687e-01,  9.6589e-01,  2.8584e-01,\n",
       "          9.7077e-01,  4.6170e-01,  5.9120e-02, -1.1430e+00,  6.7740e-01,\n",
       "         -1.8140e+00,  9.1573e-01, -1.2449e+00, -4.1536e-01,  4.5505e-01,\n",
       "         -5.5342e-01,  3.4498e-01,  1.4879e+00, -2.0870e-02, -2.0048e-01,\n",
       "          1.1585e+00, -1.3911e+00,  5.7082e-01,  2.2867e+00,  2.0307e-01,\n",
       "          7.6191e-01,  2.4966e+00, -5.1848e-01,  2.2921e+00, -6.8064e-01,\n",
       "          6.5615e-01, -1.1044e+00,  4.3319e-01,  1.1124e+00, -5.9197e-01,\n",
       "          1.0464e+00,  5.0568e-01,  2.9834e-01,  1.0643e+00,  2.6972e-01,\n",
       "          3.8817e-01, -6.5074e-01,  9.8157e-01,  6.5454e-01,  1.8615e+00,\n",
       "          9.0841e-01, -1.8989e-01, -3.1742e-01,  1.0460e+00,  1.4419e+00,\n",
       "         -1.0938e+00,  6.8302e-01,  1.9399e-01,  1.4010e+00, -4.8767e-01,\n",
       "          3.3831e-01,  6.7032e-01,  6.9528e-01,  3.5181e-01,  9.2955e-01,\n",
       "          3.7007e-01,  6.9376e-01,  2.5370e-01, -1.5957e-01,  1.9703e+00,\n",
       "          7.3957e-01,  1.6579e-01,  1.4451e+00,  8.2152e-01,  4.1295e-01,\n",
       "          1.0874e+00,  7.8716e-01,  6.9628e-01,  1.8347e+00, -7.6147e-01,\n",
       "         -1.9040e-01, -9.0110e-01,  7.8667e-01,  4.9605e-01,  1.1742e+00,\n",
       "         -6.1627e-03,  3.0433e-01,  1.4771e+00,  5.0755e-02,  1.3411e-01,\n",
       "          1.1961e+00,  1.1568e+00,  1.9585e+00,  4.9686e-01,  3.6575e-01,\n",
       "         -4.6880e-02,  1.0097e+00,  7.0230e-01, -1.3748e+00,  4.9610e-01,\n",
       "         -2.3105e-01,  7.2980e-01, -1.3978e+00, -9.5886e-01,  2.7091e-01,\n",
       "          1.8590e+00, -6.1989e-01,  9.9893e-01,  2.3337e+00, -5.4185e-02,\n",
       "         -3.7998e-01,  9.4641e-01, -1.0031e+00,  1.3042e+00, -1.7185e+00,\n",
       "         -9.4857e-02, -1.1761e-02, -9.9309e-01,  1.7341e+00,  1.2047e-01,\n",
       "         -1.8262e+00, -1.1968e+00,  4.6947e-01,  1.5607e+00,  1.5378e+00,\n",
       "         -9.2798e-01,  2.2416e-01,  1.0191e+00,  2.0207e+00, -1.1241e+00,\n",
       "          1.3956e+00,  3.9834e-01, -3.5801e-01, -1.2682e+00,  8.2691e-01,\n",
       "         -2.6704e-01,  2.7057e+00,  2.2246e+00,  1.5259e+00, -1.3053e+00,\n",
       "          1.9249e+00,  8.3312e-01,  2.7467e-01,  1.4486e-01,  3.6161e-01,\n",
       "          1.5164e+00,  9.4256e-01, -3.6828e-01,  1.0480e+00,  1.1060e+00,\n",
       "          7.6439e-01,  1.5509e+00,  2.4831e+00, -6.0414e-01, -3.1343e-01,\n",
       "          9.0865e-01, -8.5199e-01, -3.1622e-01,  5.2244e-01,  1.5322e+00,\n",
       "         -2.7871e-01,  6.6315e-01,  1.0436e+00, -3.7490e-01, -5.3476e-01,\n",
       "          1.5721e-01,  1.7793e-01, -2.4386e-01,  1.7925e+00,  2.3826e-01,\n",
       "          4.4338e-01, -1.1265e+00,  1.0127e+00, -8.9021e-01, -2.7993e+00,\n",
       "          1.2361e+00,  1.6636e+00, -5.4385e-01, -3.1764e-01,  1.5573e+00,\n",
       "          9.6841e-01, -2.4530e-01,  1.3334e+00,  5.6463e-01,  2.5762e-01,\n",
       "          2.6117e-01, -5.6503e-01, -7.8604e-01, -1.6258e+00,  5.4174e-01,\n",
       "         -7.6638e-01,  5.9553e-01,  5.8635e-01, -1.9162e-01, -3.3662e-01,\n",
       "         -8.1916e-01,  6.6983e-01,  1.7322e-01,  1.5167e+00,  2.2812e+00,\n",
       "         -5.0088e-01, -8.3968e-01,  2.4673e+00,  1.3583e+00,  8.2047e-01,\n",
       "          1.1236e+00, -1.7523e-01,  1.0418e+00, -1.4978e+00,  1.2196e-01,\n",
       "          1.0761e+00,  5.6032e-01,  1.0132e+00,  8.9244e-02, -2.4430e+00,\n",
       "          6.2448e-01,  6.3911e-01,  9.3791e-01,  6.4440e-01,  5.6243e-01,\n",
       "          3.0879e-01,  1.9938e+00,  7.2807e-02,  9.4252e-01, -3.6976e-01,\n",
       "         -9.6698e-01, -1.8745e+00, -7.7014e-01,  1.1546e-03,  2.2027e+00,\n",
       "         -6.6572e-01, -5.0743e-01, -7.5824e-02, -2.4755e+00,  2.5646e-01,\n",
       "          1.1005e-01, -5.6129e-02,  4.4136e-01,  7.3889e-01, -4.8135e-01,\n",
       "         -7.1881e-01, -1.1872e+00,  5.8204e-01,  3.4636e-01, -9.3713e-01,\n",
       "         -4.6233e-01, -1.4145e+00,  1.1157e+00,  4.0722e-01, -2.6217e-01,\n",
       "         -2.7029e-01, -1.0204e-01, -8.7406e-01, -1.0406e+00,  4.1597e-01,\n",
       "         -1.0590e+00,  1.8065e-02, -9.3697e-01, -3.6339e-01, -8.8851e-01,\n",
       "          4.2896e-02,  2.9470e-01, -1.9369e-02, -1.1689e+00, -1.7800e+00,\n",
       "         -1.0165e+00,  6.2059e-01, -8.4154e-01,  1.1002e+00,  7.6994e-01,\n",
       "          7.5688e-01,  6.8509e-01,  1.4057e-01, -2.6432e-01, -1.8584e+00,\n",
       "          8.3723e-01, -1.3150e+00,  1.7742e+00,  5.7521e-01, -9.7246e-01,\n",
       "         -4.9369e-01, -6.4272e-01,  6.6409e-01,  1.1049e-01, -3.6918e-01,\n",
       "         -1.8504e+00, -1.9948e+00,  2.4790e+00, -2.9008e-01, -9.3139e-01,\n",
       "         -3.6499e-02, -1.2521e+00, -5.1251e-01, -1.4042e+00, -2.0003e-01,\n",
       "          2.1953e-01,  2.6888e-01, -5.0403e-01,  1.6813e+00,  1.3771e+00]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1139e-04, 2.1405e-04, 1.8058e-04, 4.0382e-05, 1.1408e-04, 4.4539e-04,\n",
       "         2.2286e-04, 1.9927e-03, 1.4232e-03, 1.6160e-04, 1.1340e-04, 1.3944e-04,\n",
       "         4.1494e-04, 1.2056e-04, 1.4306e-04, 1.9693e-04, 2.1429e-04, 4.5687e-04,\n",
       "         1.9114e-04, 2.5742e-04, 1.2640e-04, 3.1875e-04, 8.9013e-05, 9.5151e-04,\n",
       "         1.6316e-04, 1.1036e-04, 1.1246e-04, 8.2232e-05, 2.7202e-04, 2.2759e-04,\n",
       "         3.3045e-04, 1.6197e-04, 3.0026e-04, 2.9940e-04, 8.8391e-04, 4.2871e-04,\n",
       "         1.0899e-03, 3.1512e-04, 2.3281e-04, 7.7068e-04, 3.7877e-04, 3.5191e-04,\n",
       "         1.8595e-04, 5.8666e-04, 2.2304e-04, 1.9574e-04, 4.9125e-04, 4.7105e-04,\n",
       "         6.7514e-05, 1.0933e-04, 1.7679e-04, 1.2845e-03, 2.3128e-04, 2.0571e-04,\n",
       "         2.7938e-04, 1.2510e-04, 3.0021e-04, 1.8548e-04, 2.3566e-04, 4.4249e-04,\n",
       "         7.3781e-04, 6.4209e-04, 7.1909e-04, 2.8506e-04, 3.0975e-04, 1.9913e-04,\n",
       "         3.2079e-04, 2.4605e-04, 1.4477e-04, 1.6455e-04, 7.3879e-05, 5.5637e-04,\n",
       "         1.1748e-04, 5.5215e-04, 1.3555e-04, 2.5900e-04, 8.4295e-04, 3.9009e-04,\n",
       "         8.8255e-04, 4.9474e-04, 3.7826e-04, 8.1069e-05, 6.3479e-04, 3.6041e-04,\n",
       "         3.2313e-04, 5.0959e-04, 8.0089e-04, 1.0095e-03, 1.0253e-03, 5.9173e-04,\n",
       "         9.8545e-05, 3.3942e-04, 6.7777e-05, 2.8115e-04, 5.9827e-04, 3.4889e-05,\n",
       "         4.3228e-04, 4.1365e-04, 5.2689e-05, 5.3510e-04, 8.7575e-05, 2.8123e-04,\n",
       "         1.2072e-04, 4.2133e-04, 2.4313e-04, 2.3352e-04, 2.5186e-04, 1.8717e-04,\n",
       "         9.6519e-05, 6.8047e-05, 9.0196e-05, 5.9886e-05, 1.1582e-03, 2.4578e-04,\n",
       "         6.3368e-04, 1.0773e-04, 1.0006e-04, 1.2083e-04, 1.5097e-03, 8.3935e-04,\n",
       "         2.3235e-04, 1.1581e-03, 1.3032e-03, 4.1288e-04, 2.2408e-03, 6.8076e-04,\n",
       "         3.4308e-04, 6.7999e-05, 1.3617e-04, 1.6403e-04, 3.3515e-05, 9.3217e-05,\n",
       "         2.6372e-04, 1.1154e-04, 2.4425e-04, 3.7481e-05, 4.2587e-04, 1.4258e-04,\n",
       "         9.4688e-05, 6.5685e-05, 1.0887e-04, 5.8810e-05, 4.5189e-05, 7.1643e-05,\n",
       "         1.2614e-04, 2.1927e-04, 1.3506e-04, 5.1264e-05, 1.3701e-04, 5.5387e-05,\n",
       "         7.4732e-04, 8.3411e-03, 2.3645e-04, 6.5704e-04, 6.2262e-04, 1.2485e-03,\n",
       "         5.8550e-04, 7.7418e-04, 8.1654e-04, 1.1512e-03, 6.9251e-04, 1.1505e-03,\n",
       "         6.9885e-04, 1.3083e-03, 6.3885e-04, 6.1902e-04, 7.4794e-04, 6.4089e-04,\n",
       "         2.1541e-03, 8.3499e-04, 3.0415e-03, 2.6527e-03, 2.0691e-03, 1.1329e-03,\n",
       "         1.3540e-03, 5.3804e-04, 2.0132e-03, 1.2804e-03, 2.2899e-03, 1.0636e-03,\n",
       "         1.6417e-03, 7.8199e-04, 1.7518e-03, 3.1708e-04, 1.4962e-03, 1.5809e-03,\n",
       "         1.9323e-03, 3.7603e-04, 7.3789e-04, 7.9171e-04, 3.5890e-04, 3.6715e-04,\n",
       "         2.1338e-03, 7.4354e-04, 4.8570e-04, 2.1344e-03, 5.7895e-04, 1.0938e-03,\n",
       "         5.3622e-04, 1.1259e-03, 7.7430e-04, 4.4181e-04, 9.2986e-04, 2.9013e-03,\n",
       "         1.7655e-03, 1.4525e-03, 1.1543e-03, 3.7331e-03, 6.0389e-03, 1.7363e-03,\n",
       "         5.7463e-04, 9.7645e-04, 2.2009e-03, 1.4979e-03, 9.1933e-04, 1.8021e-03,\n",
       "         1.0232e-03, 1.4496e-03, 1.2222e-03, 1.6843e-03, 8.4379e-04, 6.1887e-04,\n",
       "         2.6477e-03, 7.0995e-04, 5.8380e-04, 1.3062e-03, 7.6651e-04, 1.9762e-03,\n",
       "         7.7283e-04, 8.9211e-04, 3.6480e-04, 1.5949e-03, 1.5421e-03, 7.4247e-04,\n",
       "         1.1660e-03, 1.1781e-03, 8.5623e-04, 9.9973e-04, 1.6323e-03, 4.0566e-04,\n",
       "         5.9627e-04, 6.2290e-04, 7.8954e-04, 1.1902e-03, 6.2165e-04, 1.0369e-03,\n",
       "         4.9178e-04, 9.2600e-04, 1.3064e-03, 8.5530e-04, 1.1096e-03, 1.5996e-03,\n",
       "         2.5278e-04, 1.6427e-03, 2.5717e-03, 3.5454e-04, 8.8545e-04, 6.1165e-03,\n",
       "         1.2897e-03, 7.8348e-04, 6.7294e-04, 3.2034e-04, 5.2433e-04, 1.0272e-03,\n",
       "         2.0088e-03, 1.7315e-03, 9.5511e-04, 1.6563e-03, 1.0368e-03, 4.5668e-04,\n",
       "         4.4733e-04, 2.1549e-04, 1.8639e-04, 1.2496e-03, 1.2964e-04, 1.3172e-04,\n",
       "         4.6619e-04, 1.3319e-04, 8.5340e-05, 4.8508e-04, 2.2573e-04, 1.5514e-03,\n",
       "         1.6116e-03, 5.3330e-04, 1.5127e-03, 1.6091e-03, 8.7599e-04, 5.7875e-04,\n",
       "         5.5341e-04, 1.3098e-04, 1.7920e-04, 2.2410e-04, 4.0241e-04, 2.8508e-04,\n",
       "         2.4830e-04, 1.3567e-04, 2.9384e-04, 5.2788e-05, 2.0654e-04, 2.8711e-04,\n",
       "         2.0261e-04, 1.8176e-04, 3.7623e-04, 4.7780e-04, 1.3835e-04, 1.7858e-04,\n",
       "         1.0791e-03, 1.9696e-04, 1.3377e-04, 3.4146e-04, 9.5991e-04, 3.5667e-04,\n",
       "         3.9957e-04, 7.0693e-04, 1.5365e-03, 3.8313e-04, 3.9462e-04, 1.1879e-04,\n",
       "         1.1624e-04, 6.0543e-04, 1.0674e-04, 8.7040e-05, 9.2460e-05, 7.4998e-05,\n",
       "         7.2706e-05, 4.6279e-05, 1.7177e-04, 3.0165e-04, 2.5355e-04, 1.9095e-04,\n",
       "         2.6902e-04, 3.4527e-04, 3.7302e-04, 5.7114e-04, 5.0378e-04, 1.5953e-04,\n",
       "         1.1786e-04, 8.7085e-04, 7.1083e-04, 2.8978e-04, 5.4575e-04, 1.3951e-03,\n",
       "         4.0167e-04, 5.5126e-04, 2.2493e-04, 1.7521e-03, 7.1194e-04, 8.2135e-05,\n",
       "         4.8904e-04, 1.7017e-04, 2.2396e-04, 1.1671e-04, 1.9715e-04, 2.7298e-04,\n",
       "         1.0170e-03, 3.1553e-03, 2.7589e-03, 7.8890e-04, 9.5669e-04, 7.7262e-04,\n",
       "         8.2690e-04, 4.7319e-04, 4.1016e-04, 4.0384e-04, 3.4821e-04, 2.5530e-04,\n",
       "         5.6254e-05, 2.1044e-04, 1.0353e-04, 1.1668e-04, 2.3325e-04, 3.9212e-04,\n",
       "         2.7602e-04, 1.0383e-03, 2.8374e-04, 2.1057e-04, 1.4004e-04, 5.1243e-04,\n",
       "         5.0202e-04, 4.6119e-04, 4.2846e-04, 1.6083e-04, 2.1538e-04, 1.9145e-04,\n",
       "         2.1354e-04, 4.8660e-04, 4.0300e-04, 1.0781e-04, 1.4688e-04, 3.3122e-04,\n",
       "         3.6071e-04, 4.3917e-04, 1.3099e-04, 4.3999e-05, 3.0591e-04, 3.4716e-04,\n",
       "         1.0520e-04, 3.5717e-04, 7.1297e-04, 1.0620e-03, 6.8010e-04, 2.1532e-03,\n",
       "         6.2566e-04, 1.1766e-05, 1.2573e-04, 2.6517e-04, 7.2378e-04, 3.6555e-04,\n",
       "         4.4851e-04, 1.5637e-03, 2.9237e-04, 5.9294e-04, 3.5677e-03, 1.5599e-03,\n",
       "         1.2385e-03, 1.5481e-03, 4.7152e-04, 6.3366e-04, 4.4267e-04, 1.4539e-03,\n",
       "         8.9813e-04, 1.5392e-03, 3.5097e-04, 8.5266e-04, 7.0099e-04, 4.0652e-04,\n",
       "         5.8866e-04, 2.8875e-03, 5.8893e-03, 1.1095e-03, 5.0572e-04, 4.4616e-04,\n",
       "         1.2640e-03, 1.1242e-03, 1.2146e-03, 2.5654e-03, 2.9989e-04, 8.8033e-04,\n",
       "         5.5411e-04, 3.0358e-04, 1.6704e-03, 7.9524e-04, 5.8572e-04, 3.4110e-04,\n",
       "         4.8454e-04, 7.2069e-04, 6.5850e-04, 8.1270e-03, 1.8763e-03, 4.7014e-04,\n",
       "         2.7239e-04, 1.8598e-03, 1.8896e-03, 5.7556e-04, 5.7675e-04, 1.7380e-03,\n",
       "         3.0121e-03, 3.2278e-03, 6.1306e-04, 1.6054e-03, 2.0389e-04, 2.4190e-03,\n",
       "         1.6595e-03, 7.0591e-03, 3.2829e-03, 4.3430e-04, 1.2703e-04, 4.3755e-04,\n",
       "         3.3972e-04, 3.3111e-03, 3.1602e-03, 5.5540e-04, 6.2368e-04, 9.7488e-04,\n",
       "         2.7092e-04, 5.6769e-04, 4.4970e-04, 1.2173e-03, 1.9315e-03, 7.3892e-04,\n",
       "         4.9502e-04, 7.0873e-04, 1.1735e-03, 2.8559e-04, 1.6494e-04, 7.1284e-04,\n",
       "         2.7030e-04, 1.6154e-03, 3.3608e-03, 1.4506e-03, 6.5047e-04, 1.0015e-03,\n",
       "         7.3009e-04, 2.9576e-04, 1.2062e-03, 3.3138e-04, 2.0263e-04, 5.9787e-04,\n",
       "         4.5870e-04, 2.1661e-03, 1.2756e-04, 4.0133e-03, 1.4925e-03, 1.0672e-03,\n",
       "         2.3690e-03, 2.2675e-03, 5.6594e-04, 7.1475e-04, 5.5790e-04, 5.2591e-04,\n",
       "         1.2238e-04, 2.2986e-04, 6.2466e-04, 1.1622e-03, 2.7998e-03, 7.7902e-03,\n",
       "         1.1373e-03, 4.1395e-04, 2.7908e-03, 9.8541e-04, 2.2947e-04, 5.4333e-04,\n",
       "         1.7013e-03, 3.0594e-03, 9.2274e-04, 2.1026e-04, 6.8187e-04, 6.0559e-04,\n",
       "         9.3668e-04, 7.5965e-04, 2.0122e-03, 7.3348e-04, 7.4022e-04, 1.6872e-04,\n",
       "         8.4905e-04, 2.3679e-04, 3.8692e-04, 4.6639e-04, 9.1198e-04, 3.2218e-03,\n",
       "         4.5139e-05, 3.8303e-03, 3.2586e-03, 7.8851e-04, 7.5791e-04, 2.8188e-03,\n",
       "         4.7358e-04, 8.4188e-05, 1.1989e-04, 5.0836e-04, 7.6974e-04, 9.0074e-04,\n",
       "         2.8267e-03, 8.2803e-04, 1.0374e-04, 2.9242e-04, 1.5781e-03, 6.8561e-04,\n",
       "         1.7467e-03, 1.9660e-03, 1.2462e-03, 3.4230e-04, 9.1226e-04, 3.4429e-04,\n",
       "         2.9252e-04, 2.1267e-04, 6.8700e-04, 9.3237e-04, 6.8278e-04, 3.5546e-04,\n",
       "         6.0442e-03, 5.5260e-04, 2.2260e-03, 1.0412e-04, 5.9437e-04, 2.7742e-04,\n",
       "         1.0234e-04, 2.8282e-03, 1.1523e-03, 4.1796e-04, 7.0644e-04, 3.8759e-04,\n",
       "         8.5477e-04, 1.6521e-03, 6.7297e-04, 1.9913e-03, 4.5526e-04, 2.0810e-03,\n",
       "         2.1090e-03, 2.0008e-03, 1.8726e-04, 1.7433e-03, 2.1617e-04, 1.7277e-03,\n",
       "         5.3200e-04, 2.9254e-04, 2.3110e-03, 7.1870e-04, 2.5980e-04, 1.7141e-03,\n",
       "         7.5110e-03, 7.8277e-04, 6.9048e-04, 6.1833e-04, 9.4474e-04, 5.3043e-04,\n",
       "         2.2243e-03, 1.5782e-04, 9.4515e-04, 3.0993e-04, 1.5217e-03, 7.2097e-04,\n",
       "         2.8162e-04, 1.9312e-03, 6.7703e-04, 1.0309e-03, 2.4853e-03, 1.1997e-03,\n",
       "         5.5834e-03, 2.0278e-03, 1.4818e-03, 8.2851e-04, 5.5421e-04, 1.3218e-03,\n",
       "         6.9011e-04, 1.4074e-04, 3.1952e-03, 4.0762e-04, 2.2987e-04, 5.4950e-04,\n",
       "         5.1114e-04, 1.4392e-03, 2.3928e-03, 3.3452e-03, 5.5576e-04, 7.4610e-04,\n",
       "         1.8166e-03, 7.1131e-04, 1.5329e-03, 6.7475e-04, 1.1965e-04, 2.0971e-03,\n",
       "         6.7036e-04, 9.6064e-03, 7.8941e-04, 2.5737e-04, 1.1532e-03, 7.8776e-04,\n",
       "         3.4497e-04, 1.6284e-04, 2.9964e-03, 6.8060e-04, 8.0117e-04, 7.9880e-04,\n",
       "         3.7422e-04, 1.1313e-03, 8.0848e-04, 4.1539e-04, 6.8465e-04, 5.2397e-04,\n",
       "         9.3596e-04, 3.1904e-04, 5.2088e-04, 4.4607e-04, 1.1696e-03, 2.8081e-04,\n",
       "         1.8227e-03, 3.9236e-03, 2.7154e-04, 4.0822e-04, 3.0919e-04, 7.1484e-04,\n",
       "         9.6119e-04, 7.2815e-04, 2.1476e-03, 3.0403e-04, 4.6358e-03, 3.9299e-03,\n",
       "         2.2920e-03, 4.5787e-04, 6.6511e-04, 7.3863e-04, 7.3417e-04, 8.2729e-04,\n",
       "         1.0743e-03, 5.1481e-05, 4.9425e-04, 2.0841e-04, 3.3586e-04, 2.8873e-04,\n",
       "         3.2968e-04, 3.1723e-03, 1.2167e-03, 1.0901e-03, 1.1608e-04, 1.6942e-03,\n",
       "         2.5801e-03, 2.2315e-04, 4.2494e-04, 2.5526e-03, 5.4533e-03, 6.7944e-04,\n",
       "         8.2889e-04, 7.4745e-04, 1.5737e-03, 9.8585e-04, 5.9831e-04, 9.3036e-04,\n",
       "         1.5478e-03, 7.8410e-04, 1.5554e-03, 9.3486e-04, 6.2504e-04, 1.8786e-04,\n",
       "         1.1599e-03, 9.6037e-05, 1.4721e-03, 1.6966e-04, 3.8891e-04, 9.2867e-04,\n",
       "         3.3876e-04, 8.3187e-04, 2.6087e-03, 5.7699e-04, 4.8213e-04, 1.8766e-03,\n",
       "         1.4658e-04, 1.0426e-03, 5.7988e-03, 7.2181e-04, 1.2622e-03, 7.1532e-03,\n",
       "         3.5080e-04, 5.8303e-03, 2.9829e-04, 1.1355e-03, 1.9526e-04, 9.0859e-04,\n",
       "         1.7920e-03, 3.2594e-04, 1.6775e-03, 9.7690e-04, 7.9397e-04, 1.7078e-03,\n",
       "         7.7157e-04, 8.6859e-04, 3.0734e-04, 1.5723e-03, 1.1337e-03, 3.7904e-03,\n",
       "         1.4613e-03, 4.8726e-04, 4.2892e-04, 1.6769e-03, 2.4915e-03, 1.9733e-04,\n",
       "         1.1664e-03, 7.1529e-04, 2.3917e-03, 3.6178e-04, 8.2634e-04, 1.1517e-03,\n",
       "         1.1808e-03, 8.3758e-04, 1.4926e-03, 8.5301e-04, 1.1790e-03, 7.5930e-04,\n",
       "         5.0227e-04, 4.2258e-03, 1.2343e-03, 6.9540e-04, 2.4993e-03, 1.3397e-03,\n",
       "         8.9038e-04, 1.7478e-03, 1.2945e-03, 1.1820e-03, 3.6899e-03, 2.7513e-04,\n",
       "         4.8702e-04, 2.3927e-04, 1.2938e-03, 9.6753e-04, 1.9062e-03, 5.8554e-04,\n",
       "         7.9873e-04, 2.5807e-03, 6.1984e-04, 6.7372e-04, 1.9486e-03, 1.8734e-03,\n",
       "         4.1764e-03, 9.6831e-04, 8.4933e-04, 5.6218e-04, 1.6172e-03, 1.1892e-03,\n",
       "         1.4899e-04, 9.6758e-04, 4.6762e-04, 1.2223e-03, 1.4561e-04, 2.2584e-04,\n",
       "         7.7248e-04, 3.7810e-03, 3.1697e-04, 1.5998e-03, 6.0780e-03, 5.5809e-04,\n",
       "         4.0291e-04, 1.5179e-03, 2.1606e-04, 2.1709e-03, 1.0566e-04, 5.3584e-04,\n",
       "         5.8227e-04, 2.1824e-04, 3.3369e-03, 6.6459e-04, 9.4865e-05, 1.7801e-04,\n",
       "         9.4215e-04, 2.8056e-03, 2.7421e-03, 2.3293e-04, 7.3720e-04, 1.6323e-03,\n",
       "         4.4442e-03, 1.9145e-04, 2.3786e-03, 8.7746e-04, 4.1186e-04, 1.6576e-04,\n",
       "         1.3470e-03, 4.5109e-04, 8.8167e-03, 5.4496e-03, 2.7096e-03, 1.5972e-04,\n",
       "         4.0383e-03, 1.3554e-03, 7.7539e-04, 6.8099e-04, 8.4582e-04, 2.6841e-03,\n",
       "         1.5121e-03, 4.0765e-04, 1.6802e-03, 1.7806e-03, 1.2653e-03, 2.7782e-03,\n",
       "         7.0572e-03, 3.2200e-04, 4.3064e-04, 1.4617e-03, 2.5132e-04, 4.2944e-04,\n",
       "         9.9340e-04, 2.7268e-03, 4.4585e-04, 1.1435e-03, 1.6728e-03, 4.0496e-04,\n",
       "         3.4514e-04, 6.8946e-04, 7.0389e-04, 4.6167e-04, 3.5376e-03, 7.4767e-04,\n",
       "         9.1789e-04, 1.9099e-04, 1.6219e-03, 2.4189e-04, 3.5852e-05, 2.0281e-03,\n",
       "         3.1097e-03, 3.4201e-04, 4.2883e-04, 2.7962e-03, 1.5517e-03, 4.6100e-04,\n",
       "         2.2353e-03, 1.0362e-03, 7.6228e-04, 7.6499e-04, 3.3484e-04, 2.6845e-04,\n",
       "         1.1592e-04, 1.0128e-03, 2.7378e-04, 1.0687e-03, 1.0590e-03, 4.8642e-04,\n",
       "         4.2077e-04, 2.5970e-04, 1.1512e-03, 7.0058e-04, 2.6849e-03, 5.7671e-03,\n",
       "         3.5703e-04, 2.5443e-04, 6.9462e-03, 2.2917e-03, 1.3383e-03, 1.8121e-03,\n",
       "         4.9446e-04, 1.6699e-03, 1.3174e-04, 6.6558e-04, 1.7282e-03, 1.0318e-03,\n",
       "         1.6227e-03, 6.4416e-04, 5.1198e-05, 1.1001e-03, 1.1163e-03, 1.5051e-03,\n",
       "         1.1223e-03, 1.0339e-03, 8.0230e-04, 4.3264e-03, 6.3366e-04, 1.5120e-03,\n",
       "         4.0705e-04, 2.2402e-04, 9.0399e-05, 2.7275e-04, 5.8984e-04, 5.3316e-03,\n",
       "         3.0277e-04, 3.5470e-04, 5.4614e-04, 4.9559e-05, 7.6140e-04, 6.5770e-04,\n",
       "         5.5700e-04, 9.1604e-04, 1.2335e-03, 3.6407e-04, 2.8712e-04, 1.7974e-04,\n",
       "         1.0544e-03, 8.3302e-04, 2.3081e-04, 3.7106e-04, 1.4320e-04, 1.7979e-03,\n",
       "         8.8529e-04, 4.5329e-04, 4.4962e-04, 5.3201e-04, 2.4583e-04, 2.0812e-04,\n",
       "         8.9307e-04, 2.0432e-04, 5.9990e-04, 2.3084e-04, 4.0965e-04, 2.4230e-04,\n",
       "         6.1498e-04, 7.9108e-04, 5.7786e-04, 1.8306e-04, 9.9358e-05, 2.1318e-04,\n",
       "         1.0958e-03, 2.5396e-04, 1.7703e-03, 1.2724e-03, 1.2559e-03, 1.1689e-03,\n",
       "         6.7808e-04, 4.5231e-04, 9.1862e-05, 1.3609e-03, 1.5818e-04, 3.4733e-03,\n",
       "         1.0472e-03, 2.2279e-04, 3.5961e-04, 3.0982e-04, 1.1446e-03, 6.5799e-04,\n",
       "         4.0729e-04, 9.2601e-05, 8.0150e-05, 7.0280e-03, 4.4081e-04, 2.3213e-04,\n",
       "         5.6804e-04, 1.6844e-04, 3.5290e-04, 1.4467e-04, 4.8235e-04, 7.3380e-04,\n",
       "         7.7091e-04, 3.5591e-04, 3.1653e-03, 2.3352e-03]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ooak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 640])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5621)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak[2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 640])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100][0].shape#.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2595],\n",
       "         [-1.2492],\n",
       "         [-1.2483],\n",
       "         ...,\n",
       "         [-1.2460],\n",
       "         [-1.2495],\n",
       "         [-1.2554]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['3']['cls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.sigmoid(s['3']['cls'])>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box loss:0.4609232246875763\n",
      "cls loss:0.38423848152160645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8452, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(s, kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box loss:0.7665533423423767\n",
      "cls loss:7.646162986755371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(8.4127, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(s, data[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 160, 160])\n",
      "tensor(0.0034, grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 32, 80, 80])\n",
      "tensor(-0.0042, grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 96, 40, 40])\n",
      "tensor(0.0060, grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 1280, 20, 20])\n",
      "tensor(0.2237, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in list(fd.raw_features.keys()):\n",
    "    print (fd.raw_features[i].shape)\n",
    "    print (fd.raw_features[i].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 20, 20])\n",
      "tensor(-0.1097, grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 256, 40, 40])\n",
      "tensor(-0.1092, grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 256, 80, 80])\n",
      "tensor(-0.1187, grad_fn=<MeanBackward0>)\n",
      "torch.Size([1, 256, 160, 160])\n",
      "tensor(-0.1175, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in list(fd.newfeatures.keys()):\n",
    "    print (fd.newfeatures[i].shape)\n",
    "    print (fd.newfeatures[i].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box loss:0.13568367063999176\n",
      "cls loss:62838.42578125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(62838.5625, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(kk, data[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5135, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inp).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
