{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from FPN import Features, FPNetwork , classificationhead , bboxhead\n",
    "from Loss import Lossfunction\n",
    "from datasets import load_dataset\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import  GradScaler\n",
    "from torch.amp import autocast\n",
    "import gc\n",
    "from dataset_convert import AnchorGenerator, FaceDetectionDataset\n",
    "# device = torch.device(\"mps\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/vipulagarwal/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "model = model.features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"CUHK-CSE/wider_face\")\n",
    "train_dataset = dataset['train'].with_format(\"torch\")\n",
    "val_dataset = dataset['validation'].with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Features(model,['3','6', '13','18'])\n",
    "topdown = FPNetwork(out_channels=256)\n",
    "classifier = classificationhead(channels=256, num_anchors= 12, num_of_classes= 1)\n",
    "bboxregression = bboxhead(channels= 256 , num_anchors= 12)\n",
    "loss =Lossfunction(lambd=10)\n",
    "anchors = AnchorGenerator()\n",
    "data = FaceDetectionDataset(train_dataset,anchors)\n",
    "vali_data = FaceDetectionDataset(val_dataset,anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.03 GB\n",
      "Cached: 0.05 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.03 GB\n",
      "Cached: 0.05 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# epochs = 20\n",
    "# learning_rate = 1e-3\n",
    "def forward(p):\n",
    "    features = extractor.extract(p)\n",
    "    newfeatures = topdown(features)\n",
    "    output = {}\n",
    "    for key in list(newfeatures.keys()):\n",
    "        temp = {}\n",
    "        temp[\"bbox\"] = bboxregression(newfeatures[key])\n",
    "        temp[\"cls\"] = classifier(newfeatures[key])\n",
    "        output[key] = temp\n",
    "    return output\n",
    "\n",
    "def train(epochs:int,training_data , validation_data=None):\n",
    "    extractor.train()\n",
    "    topdown.train()\n",
    "    classifier.train()\n",
    "    bboxregression.train()\n",
    "    optimizer = optim.Adam(list(extractor.parameters())+\n",
    "                           list(topdown.parameters())+\n",
    "                           list(classifier.parameters())+\n",
    "                           list(bboxregression.parameters()), lr=learning_rate)\n",
    "    loSS = {}\n",
    "    for i in range (0,epochs):\n",
    "        train_loss= 0\n",
    "        for key, (image,bbox) in enumerate(training_data):\n",
    "            epoch_loss = {}\n",
    "            model_pred = forward(image)\n",
    "            optimizer.zero_grad()\n",
    "            ll = loss(model_pred, bbox)\n",
    "            train_loss+=ll\n",
    "            if (key % 10 == 0):\n",
    "                epoch_loss[key] = train_loss/10\n",
    "                print(f\"The avg loss for {key}th data is {train_loss/10}\")\n",
    "                train_loss = 0    \n",
    "            ll.backward()\n",
    "            optimizer.step()\n",
    "        loSS[i] = epoch_loss\n",
    "    return loSS\n",
    "\n",
    "# train(epochs=epochs,training_data=training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7454644\n"
     ]
    }
   ],
   "source": [
    "total_params=sum(p.numel() for p in extractor.parameters())\n",
    "total_params+= sum(p.numel() for p in topdown.parameters())\n",
    "total_params+= sum(p.numel() for p in classifier.parameters())\n",
    "total_params+= sum(p.numel() for p in bboxregression.parameters())\n",
    "\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_266/4292142752.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_accumulation(epochs: int, training_data, validation_data=None, accumulation_steps=4):\n",
    "    # Set models to training mode\n",
    "    extractor.train()\n",
    "    topdown.train()\n",
    "    classifier.train()\n",
    "    bboxregression.train()\n",
    "    \n",
    "    # Enable gradient checkpointing\n",
    "    if hasattr(extractor, 'gradient_checkpointing_enable'):\n",
    "        extractor.gradient_checkpointing_enable()\n",
    "    if hasattr(topdown, 'gradient_checkpointing_enable'):\n",
    "        topdown.gradient_checkpointing_enable()\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        list(extractor.parameters()) +\n",
    "        list(topdown.parameters()) +\n",
    "        list(classifier.parameters()) +\n",
    "        list(bboxregression.parameters()), \n",
    "        lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    loss_history = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        batch_count = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        try:\n",
    "            for batch_idx, (image, bbox) in enumerate(training_data):\n",
    "                # Mixed precision forward pass\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with autocast('cuda'):\n",
    "                    model_pred = forward(image.cuda())\n",
    "                    ll = loss(model_pred, bbox) / accumulation_steps  # Scale loss\n",
    "                del image , model_pred , bbox\n",
    "                # Backward pass\n",
    "                scaler.scale(ll).backward()\n",
    "                \n",
    "                # Step optimizer every accumulation_steps\n",
    "                if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                # Accumulate loss\n",
    "                batch_loss = ll.item() * accumulation_steps  # Unscale for logging\n",
    "                epoch_loss += batch_loss\n",
    "                running_loss += batch_loss\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Print progress\n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    avg_running_loss = running_loss / 10\n",
    "                    print(\"-\" * 30)\n",
    "                    print(f\"Batch {batch_idx + 1}: Avg Loss = {avg_running_loss:.6f}\")\n",
    "                    print(\"-\" * 30)\n",
    "                    running_loss = 0.0\n",
    "                \n",
    "                # Memory \n",
    "                del ll\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        except Exception as e :\n",
    "            print (e)\n",
    "            print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "            print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "            print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "            break\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"INTERRUPTED!!\")\n",
    "            return loss_history   \n",
    "\n",
    "                \n",
    "        if batch_count % accumulation_steps != 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / batch_count if batch_count > 0 else 0\n",
    "        loss_history[epoch] = avg_epoch_loss\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.6f}\")\n",
    "        \n",
    "        # Cleanup at epoch end\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 8\n",
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "training_data = DataLoader(\n",
    "    data,\n",
    "    batch_size=batch_size, \n",
    "    num_workers=4,     # Reduce if GPU memory is full\n",
    "    pin_memory=True,         # Faster CPU->GPU transfer\n",
    "    persistent_workers=True, # Reuse workers across epochs\n",
    "    prefetch_factor=4,       # Prefetch more batches\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "validation_data = DataLoader(vali_data,\n",
    "    batch_size=batch_size, \n",
    "    num_workers=4,     # Reduce if GPU memory is full\n",
    "    pin_memory=True,         # Faster CPU->GPU transfer\n",
    "    persistent_workers=True, # Reuse workers across epochs\n",
    "    prefetch_factor=4,       # Prefetch more batches\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n",
      "------------------------------\n",
      "box loss:3.5145514011383057\n",
      "cls loss:3659.02685546875\n",
      "box loss:2.7041258811950684\n",
      "cls loss:2951.58447265625\n",
      "box loss:2.1685640811920166\n",
      "cls loss:13842.23046875\n",
      "box loss:2.8785951137542725\n",
      "cls loss:3645.6484375\n",
      "box loss:2.2008285522460938\n",
      "cls loss:2288.5556640625\n",
      "box loss:6.365060806274414\n",
      "cls loss:8771.6748046875\n",
      "box loss:3.5769691467285156\n",
      "cls loss:5418.03125\n",
      "box loss:2.6229751110076904\n",
      "cls loss:1269.349365234375\n",
      "box loss:1.5934503078460693\n",
      "cls loss:1104.1712646484375\n",
      "box loss:1.130946397781372\n",
      "cls loss:2296.52392578125\n",
      "------------------------------\n",
      "Batch 10: Avg Loss = 4553.435669\n",
      "------------------------------\n",
      "box loss:0.7891912460327148\n",
      "cls loss:3107.992431640625\n",
      "box loss:1.3655290603637695\n",
      "cls loss:2027.667236328125\n",
      "box loss:0.9878318905830383\n",
      "cls loss:1096.649169921875\n",
      "box loss:0.784155011177063\n",
      "cls loss:4046.7470703125\n",
      "box loss:4.194526195526123\n",
      "cls loss:5035.4296875\n",
      "box loss:1.036190152168274\n",
      "cls loss:1054.8594970703125\n",
      "box loss:1.181241512298584\n",
      "cls loss:3412.980712890625\n",
      "box loss:3.598207712173462\n",
      "cls loss:3106.58349609375\n",
      "box loss:1.515334129333496\n",
      "cls loss:4003.9580078125\n",
      "box loss:0.7031501531600952\n",
      "cls loss:2624.18798828125\n",
      "------------------------------\n",
      "Batch 20: Avg Loss = 2967.860901\n",
      "------------------------------\n",
      "box loss:0.6775346994400024\n",
      "cls loss:836.4931640625\n",
      "box loss:1.6374340057373047\n",
      "cls loss:3559.908935546875\n",
      "box loss:0.46740251779556274\n",
      "cls loss:2052.217041015625\n",
      "box loss:1.3329691886901855\n",
      "cls loss:852.9842529296875\n",
      "box loss:1.0325974225997925\n",
      "cls loss:1003.2671508789062\n",
      "box loss:2.7766966819763184\n",
      "cls loss:4478.205078125\n",
      "box loss:4.145995140075684\n",
      "cls loss:2194.98779296875\n",
      "box loss:0.43398332595825195\n",
      "cls loss:5419.9326171875\n",
      "box loss:0.46046048402786255\n",
      "cls loss:1426.22900390625\n",
      "box loss:1.358723521232605\n",
      "cls loss:2773.40478515625\n",
      "------------------------------\n",
      "Batch 30: Avg Loss = 2474.086786\n",
      "------------------------------\n",
      "box loss:1.0941308736801147\n",
      "cls loss:4450.0107421875\n",
      "box loss:0.5243419408798218\n",
      "cls loss:13130.853515625\n",
      "box loss:0.7927586436271667\n",
      "cls loss:2582.3388671875\n",
      "box loss:0.45277366042137146\n",
      "cls loss:2041.387451171875\n",
      "box loss:0.8875120878219604\n",
      "cls loss:1115.800537109375\n",
      "box loss:1.2553640604019165\n",
      "cls loss:16791.744140625\n",
      "box loss:0.6183817386627197\n",
      "cls loss:7072.8828125\n",
      "box loss:1.993229627609253\n",
      "cls loss:1181.99072265625\n",
      "box loss:4.8364973068237305\n",
      "cls loss:3473.93359375\n",
      "box loss:0.45041996240615845\n",
      "cls loss:1899.88330078125\n",
      "------------------------------\n",
      "Batch 40: Avg Loss = 5386.987830\n",
      "------------------------------\n",
      "box loss:1.4181885719299316\n",
      "cls loss:1125.82568359375\n",
      "box loss:0.4413289725780487\n",
      "cls loss:1205.419189453125\n",
      "box loss:4.122342586517334\n",
      "cls loss:6345.8251953125\n",
      "box loss:1.020542860031128\n",
      "cls loss:545.473388671875\n",
      "box loss:1.1189229488372803\n",
      "cls loss:2448.945556640625\n",
      "box loss:3.4612743854522705\n",
      "cls loss:4956.06201171875\n",
      "box loss:2.1275970935821533\n",
      "cls loss:1187.887451171875\n",
      "box loss:0.8482874631881714\n",
      "cls loss:1803.298095703125\n",
      "box loss:0.5805715918540955\n",
      "cls loss:1641.34765625\n",
      "box loss:1.139810562133789\n",
      "cls loss:2541.3955078125\n",
      "------------------------------\n",
      "Batch 50: Avg Loss = 2396.426880\n",
      "------------------------------\n",
      "box loss:1.4838253259658813\n",
      "cls loss:922.7418823242188\n",
      "box loss:1.102920651435852\n",
      "cls loss:2991.1279296875\n",
      "box loss:1.2265416383743286\n",
      "cls loss:2075.89794921875\n",
      "box loss:1.3261548280715942\n",
      "cls loss:2404.327880859375\n",
      "box loss:2.511012077331543\n",
      "cls loss:3765.25927734375\n",
      "box loss:0.44117599725723267\n",
      "cls loss:1469.63037109375\n",
      "box loss:2.2915749549865723\n",
      "cls loss:639.5379638671875\n",
      "box loss:0.4960061311721802\n",
      "cls loss:1950.29833984375\n",
      "box loss:0.3943856358528137\n",
      "cls loss:3126.09326171875\n",
      "box loss:0.43969666957855225\n",
      "cls loss:927.9998168945312\n",
      "------------------------------\n",
      "Batch 60: Avg Loss = 2039.004761\n",
      "------------------------------\n",
      "box loss:4.1096296310424805\n",
      "cls loss:1668.76806640625\n",
      "box loss:0.4602847695350647\n",
      "cls loss:6062.6142578125\n",
      "box loss:2.2845828533172607\n",
      "cls loss:1346.1456298828125\n",
      "box loss:0.5628467798233032\n",
      "cls loss:3154.28564453125\n",
      "box loss:0.7726569175720215\n",
      "cls loss:2051.533935546875\n",
      "box loss:0.4556673765182495\n",
      "cls loss:1732.1456298828125\n",
      "box loss:0.6610034704208374\n",
      "cls loss:2512.16162109375\n",
      "box loss:1.2967991828918457\n",
      "cls loss:2091.603759765625\n",
      "box loss:0.6616031527519226\n",
      "cls loss:1222.4981689453125\n",
      "box loss:0.9782055616378784\n",
      "cls loss:4024.161865234375\n",
      "------------------------------\n",
      "Batch 70: Avg Loss = 2598.835156\n",
      "------------------------------\n",
      "box loss:1.0564634799957275\n",
      "cls loss:1117.648681640625\n",
      "box loss:2.202727794647217\n",
      "cls loss:5196.34033203125\n",
      "box loss:2.075181484222412\n",
      "cls loss:1825.65625\n",
      "box loss:0.9208674430847168\n",
      "cls loss:553.4144287109375\n",
      "box loss:1.539899468421936\n",
      "cls loss:1457.39208984375\n",
      "box loss:1.1566141843795776\n",
      "cls loss:1766.5570068359375\n",
      "box loss:1.2113173007965088\n",
      "cls loss:3305.856689453125\n",
      "box loss:0.4660912752151489\n",
      "cls loss:1620.2916259765625\n",
      "box loss:0.8534526824951172\n",
      "cls loss:1801.52734375\n",
      "box loss:0.43495720624923706\n",
      "cls loss:2878.021484375\n",
      "------------------------------\n",
      "Batch 80: Avg Loss = 2164.188190\n",
      "------------------------------\n",
      "box loss:0.829255998134613\n",
      "cls loss:2264.5087890625\n",
      "box loss:1.6669522523880005\n",
      "cls loss:4988.46484375\n",
      "box loss:0.5849639177322388\n",
      "cls loss:1241.16259765625\n",
      "box loss:1.1683642864227295\n",
      "cls loss:4005.045654296875\n",
      "box loss:0.3511207103729248\n",
      "cls loss:2922.623046875\n",
      "box loss:0.5466387867927551\n",
      "cls loss:5746.076171875\n",
      "box loss:1.273228406906128\n",
      "cls loss:1805.19921875\n",
      "box loss:0.43199700117111206\n",
      "cls loss:2911.82080078125\n",
      "box loss:0.49510738253593445\n",
      "cls loss:1029.1146240234375\n",
      "box loss:1.344916582107544\n",
      "cls loss:1875.659423828125\n",
      "------------------------------\n",
      "Batch 90: Avg Loss = 2887.660046\n",
      "------------------------------\n",
      "box loss:0.41532257199287415\n",
      "cls loss:1931.181396484375\n",
      "box loss:0.9150936603546143\n",
      "cls loss:3615.995361328125\n",
      "box loss:0.43642669916152954\n",
      "cls loss:1353.439453125\n",
      "box loss:1.0690455436706543\n",
      "cls loss:805.9656372070312\n",
      "box loss:1.737642765045166\n",
      "cls loss:5553.6025390625\n",
      "box loss:0.42896780371665955\n",
      "cls loss:1591.529052734375\n",
      "box loss:1.3564479351043701\n",
      "cls loss:1982.556884765625\n",
      "box loss:0.3764635920524597\n",
      "cls loss:2190.064453125\n",
      "box loss:0.4170989990234375\n",
      "cls loss:1760.7882080078125\n",
      "box loss:0.41927286982536316\n",
      "cls loss:1443.53466796875\n",
      "------------------------------\n",
      "Batch 100: Avg Loss = 2230.437543\n",
      "------------------------------\n",
      "box loss:1.5318071842193604\n",
      "cls loss:1670.9034423828125\n",
      "box loss:0.5313882231712341\n",
      "cls loss:687.7088623046875\n",
      "box loss:0.9320070147514343\n",
      "cls loss:1517.535888671875\n",
      "box loss:0.48976635932922363\n",
      "cls loss:2228.150146484375\n",
      "box loss:0.8291248679161072\n",
      "cls loss:1616.2237548828125\n",
      "box loss:2.6759192943573\n",
      "cls loss:1700.04736328125\n",
      "box loss:0.40649741888046265\n",
      "cls loss:3838.0439453125\n",
      "box loss:0.7339666485786438\n",
      "cls loss:4535.99365234375\n",
      "box loss:0.800971269607544\n",
      "cls loss:1243.7769775390625\n",
      "box loss:1.0785534381866455\n",
      "cls loss:1514.349853515625\n",
      "------------------------------\n",
      "Batch 110: Avg Loss = 2065.283417\n",
      "------------------------------\n",
      "box loss:0.4724738597869873\n",
      "cls loss:1820.33447265625\n",
      "box loss:0.6879382133483887\n",
      "cls loss:2055.705078125\n",
      "box loss:1.4268298149108887\n",
      "cls loss:3454.494140625\n",
      "box loss:1.0602611303329468\n",
      "cls loss:1336.3094482421875\n",
      "box loss:1.150160551071167\n",
      "cls loss:1482.7528076171875\n",
      "box loss:1.2323646545410156\n",
      "cls loss:1846.447998046875\n",
      "box loss:0.9522702693939209\n",
      "cls loss:2093.76513671875\n",
      "box loss:1.6990915536880493\n",
      "cls loss:3373.056884765625\n",
      "box loss:0.41187745332717896\n",
      "cls loss:2891.459228515625\n",
      "box loss:1.4323704242706299\n",
      "cls loss:8465.833984375\n",
      "------------------------------\n",
      "Batch 120: Avg Loss = 2892.541528\n",
      "------------------------------\n",
      "box loss:0.8361847400665283\n",
      "cls loss:924.0805053710938\n",
      "box loss:2.6135263442993164\n",
      "cls loss:1990.13671875\n",
      "box loss:1.0287150144577026\n",
      "cls loss:2402.818115234375\n",
      "box loss:0.5882631540298462\n",
      "cls loss:916.513671875\n",
      "box loss:0.9673454165458679\n",
      "cls loss:1651.908447265625\n",
      "box loss:2.3713812828063965\n",
      "cls loss:4005.83251953125\n",
      "box loss:1.8130533695220947\n",
      "cls loss:851.9265747070312\n",
      "box loss:1.038198709487915\n",
      "cls loss:1094.90673828125\n",
      "box loss:0.7509866952896118\n",
      "cls loss:8547.1591796875\n",
      "box loss:0.6042248606681824\n",
      "cls loss:3877.423828125\n",
      "------------------------------\n",
      "Batch 130: Avg Loss = 2638.882501\n",
      "------------------------------\n",
      "box loss:0.626061201095581\n",
      "cls loss:3149.28857421875\n",
      "box loss:0.5823315978050232\n",
      "cls loss:854.611328125\n",
      "box loss:1.4497137069702148\n",
      "cls loss:1697.026123046875\n",
      "box loss:0.4009881615638733\n",
      "cls loss:4341.21875\n",
      "box loss:0.5058861374855042\n",
      "cls loss:3248.06005859375\n",
      "box loss:2.4448277950286865\n",
      "cls loss:1819.78564453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f04cc302e10>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/faces2.0/.conda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box loss:0.662362277507782\n",
      "cls loss:2507.304931640625\n",
      "box loss:0.586776614189148\n",
      "cls loss:1486.951904296875\n",
      "box loss:0.4254346489906311\n",
      "cls loss:1295.425537109375\n",
      "box loss:1.0042507648468018\n",
      "cls loss:2928.48291015625\n",
      "------------------------------\n",
      "Batch 140: Avg Loss = 2341.504181\n",
      "------------------------------\n",
      "box loss:0.4239314794540405\n",
      "cls loss:1207.3135986328125\n",
      "box loss:0.44571420550346375\n",
      "cls loss:5920.3251953125\n",
      "box loss:2.1272997856140137\n",
      "cls loss:2449.819580078125\n",
      "box loss:0.4631236791610718\n",
      "cls loss:1984.474609375\n",
      "DataLoader worker (pid(s) 3123, 3124, 3125, 3126) exited unexpectedly\n",
      "Allocated: 1.85 GB\n",
      "Cached: 2.57 GB\n",
      "Allocated: 1.85 GB\n",
      "Cached: 2.57 GB\n"
     ]
    }
   ],
   "source": [
    "lossdata = train_with_accumulation(epochs = 1, training_data = training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2135, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2135, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2264, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2173, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2080, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2256, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2224, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2076, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     10\u001b[39m     model_pred = forward(image.cuda())\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     ll = \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m / accumulation_steps  \u001b[38;5;66;03m# Scale loss\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m image , model_pred , bbox\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(ll)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/faces2.0/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/faces2.0/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/faces2.0/Loss.py:33\u001b[39m, in \u001b[36mLossfunction.forward\u001b[39m\u001b[34m(self, predicted, targets)\u001b[39m\n\u001b[32m     30\u001b[39m predicta = predicted[key]\n\u001b[32m     31\u001b[39m targeta = targets[level]\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m cls_loss += \u001b[38;5;28mself\u001b[39m.classloss(predicta[\u001b[33m\"\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m\"\u001b[39m], \u001b[43mtargeta\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcls_targets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m targeta[\u001b[33m'\u001b[39m\u001b[33mbbox_weights\u001b[39m\u001b[33m'\u001b[39m].sum() > \u001b[32m0\u001b[39m:\n\u001b[32m     35\u001b[39m     bbox_loss += \u001b[38;5;28mself\u001b[39m.reg_loss(predicta[\u001b[33m\"\u001b[39m\u001b[33mbbox\u001b[39m\u001b[33m\"\u001b[39m],targeta[\u001b[33m\"\u001b[39m\u001b[33mbbox_targets\u001b[39m\u001b[33m\"\u001b[39m].to(device),targeta[\u001b[33m'\u001b[39m\u001b[33mbbox_weights\u001b[39m\u001b[33m'\u001b[39m].to(device))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "loss = Lossfunction()\n",
    "extractor.eval()\n",
    "topdown.eval()\n",
    "classifier.eval()\n",
    "bboxregression.eval()\n",
    "accumulation_steps = 4\n",
    "for batch_idx, (image, bbox) in enumerate(validation_data):\n",
    "\n",
    "    with autocast('cuda'):\n",
    "        model_pred = forward(image.cuda())\n",
    "        ll = loss(model_pred, bbox) / accumulation_steps  # Scale loss\n",
    "    del image , model_pred , bbox\n",
    "    print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 2.11 GB\n",
      "Cached: 2.21 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 640, 640])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1000][0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=forward(data[100][0].unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= data[100][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19200, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1]['bbox_targets'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(111445.5469, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(s['3']['bbox'].cpu()-t[3]['bbox_targets']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(613)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(s['3']['cls'].cpu()>0.55).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(163553.8438, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['3']['cls'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]['cls_targets'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box loss:0.19899043440818787\n",
      "cls loss:78787.1875\n"
     ]
    }
   ],
   "source": [
    "ls = loss(s,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9902, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4933, 0.5469, 0.4821, 0.4534],\n",
       "        [0.4507, 0.4666, 0.4542, 0.4803],\n",
       "        [0.4810, 0.4796, 0.4658, 0.4742],\n",
       "        ...,\n",
       "        [0.4389, 0.0000, 0.4425, 0.0434],\n",
       "        [0.4311, 0.0000, 0.4414, 0.0056],\n",
       "        [0.4147, 0.0000, 0.3418, 0.0000]], grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['18']['bbox'].squeeze(0).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1751, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(data[1000][1][0]['bbox_targets'] - s['18']['bbox'].squeeze(0).cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model3 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "model3 = model3.features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m torch.all((\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[31mRuntimeError\u001b[39m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "torch.all(list(extractor.parameters()) == list(model3.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(extractor.state_dict(),'/home/faces2.0/models/extractor.pt')\n",
    "torch.save(topdown.state_dict(),'/home/faces2.0/models/topdown.pt')\n",
    "torch.save(classifier.state_dict(),'/home/faces2.0/models/classifier.pt')\n",
    "torch.save(bboxregression.state_dict(),'/home/faces2.0/models/bboxregression.pt')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
