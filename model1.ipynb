{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from FPN import Features, FPNetwork , classificationhead , bboxhead\n",
    "from Loss import Lossfunction\n",
    "from datasets import load_dataset\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import gc\n",
    "from dataset_convert import AnchorGenerator, FaceDetectionDataset\n",
    "# device = torch.device(\"mps\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/faces2.0/.conda/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/faces2.0/.conda/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "model = model.features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"CUHK-CSE/wider_face\")\n",
    "train_dataset = dataset['train'].with_format(\"torch\")\n",
    "val_dataset = dataset['validation'].with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Features(model,['3','6', '13','18'])\n",
    "topdown = FPNetwork(out_channels=256)\n",
    "classifier = classificationhead(channels=256, num_anchors= 12, num_of_classes= 1)\n",
    "bboxregression = bboxhead(channels= 256 , num_anchors= 12)\n",
    "loss =Lossfunction()\n",
    "anchors = AnchorGenerator()\n",
    "data = FaceDetectionDataset(train_dataset,anchors)\n",
    "vali_data = FaceDetectionDataset(val_dataset,anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparametrs\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = DataLoader(\n",
    "    data,\n",
    "    batch_size=8,           # Reduce if GPU memory is full\n",
    "    num_workers=4,           # Use 4-8 workers for CPU-bound tasks\n",
    "    # pin_memory=True,         # Faster CPU->GPU transfer\n",
    "    persistent_workers=True, # Reuse workers across epochs\n",
    "    prefetch_factor=4,       # Prefetch more batches\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "# training_data = DataLoader(\n",
    "#     data,\n",
    "#     batch_size=8,           # Reduce if GPU memory is full\n",
    "#     num_workers=4,           # Use 4-8 workers for CPU-bound tasks\n",
    "#     # pin_memory=True,         # Faster CPU->GPU transfer\n",
    "#     persistent_workers=True, # Reuse workers across epochs\n",
    "#     prefetch_factor=4,       # Prefetch more batches\n",
    "#     drop_last=True,\n",
    "#     shuffle=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = DataLoader(vali_data, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.04 GB\n",
      "Cached: 0.05 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# epochs = 20\n",
    "# learning_rate = 1e-3\n",
    "def forward(p):\n",
    "    features = extractor.extract(p)\n",
    "    newfeatures = topdown(features)\n",
    "    output = {}\n",
    "    for key in list(newfeatures.keys()):\n",
    "        temp = {}\n",
    "        temp[\"bbox\"] = bboxregression(newfeatures[key])\n",
    "        temp[\"cls\"] = classifier(newfeatures[key])\n",
    "        output[key] = temp\n",
    "    return output\n",
    "\n",
    "def train(epochs:int,training_data , validation_data=None):\n",
    "    extractor.train()\n",
    "    topdown.train()\n",
    "    classifier.train()\n",
    "    bboxregression.train()\n",
    "    optimizer = optim.Adam(list(extractor.parameters())+\n",
    "                           list(topdown.parameters())+\n",
    "                           list(classifier.parameters())+\n",
    "                           list(bboxregression.parameters()), lr=learning_rate)\n",
    "    loSS = {}\n",
    "    for i in range (0,epochs):\n",
    "        train_loss= 0\n",
    "        for key, (image,bbox) in enumerate(training_data):\n",
    "            epoch_loss = {}\n",
    "            model_pred = forward(image)\n",
    "            optimizer.zero_grad()\n",
    "            ll = loss(model_pred, bbox)\n",
    "            train_loss+=ll\n",
    "            if (key % 10 == 0):\n",
    "                epoch_loss[key] = train_loss/10\n",
    "                print(f\"The avg loss for {key}th data is {train_loss/10}\")\n",
    "                train_loss = 0    \n",
    "            ll.backward()\n",
    "            optimizer.step()\n",
    "        loSS[i] = epoch_loss\n",
    "    return loSS\n",
    "\n",
    "# train(epochs=epochs,training_data=training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7452536\n"
     ]
    }
   ],
   "source": [
    "total_params=sum(p.numel() for p in extractor.parameters())\n",
    "total_params+= sum(p.numel() for p in topdown.parameters())\n",
    "total_params+= sum(p.numel() for p in classifier.parameters())\n",
    "total_params+= sum(p.numel() for p in bboxregression.parameters())\n",
    "\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(epochs=epochs,training_data=training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7015/4292142752.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_accumulation(epochs: int, training_data, validation_data=None, accumulation_steps=4):\n",
    "    # Set models to training mode\n",
    "    extractor.train()\n",
    "    topdown.train()\n",
    "    classifier.train()\n",
    "    bboxregression.train()\n",
    "    \n",
    "    # Enable gradient checkpointing\n",
    "    if hasattr(extractor, 'gradient_checkpointing_enable'):\n",
    "        extractor.gradient_checkpointing_enable()\n",
    "    if hasattr(topdown, 'gradient_checkpointing_enable'):\n",
    "        topdown.gradient_checkpointing_enable()\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        list(extractor.parameters()) +\n",
    "        list(topdown.parameters()) +\n",
    "        list(classifier.parameters()) +\n",
    "        list(bboxregression.parameters()), \n",
    "        lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    loss_history = {}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        batch_count = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for batch_idx, (image, bbox) in enumerate(training_data):\n",
    "            # Mixed precision forward pass\n",
    "            with autocast():\n",
    "                model_pred = forward(image)\n",
    "                ll = loss(model_pred, bbox) / accumulation_steps  # Scale loss\n",
    "            \n",
    "            # Backward pass\n",
    "            scaler.scale(ll).backward()\n",
    "            \n",
    "            # Step optimizer every accumulation_steps\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            batch_loss = ll.item() * accumulation_steps  # Unscale for logging\n",
    "            epoch_loss += batch_loss\n",
    "            running_loss += batch_loss\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                avg_running_loss = running_loss / 10\n",
    "                print(f\"Batch {batch_idx + 1}: Avg Loss = {avg_running_loss:.6f}\")\n",
    "                running_loss = 0.0\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del model_pred, ll\n",
    "            if batch_idx % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "            break\n",
    "        # Handle any remaining gradients\n",
    "        if batch_count % accumulation_steps != 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / batch_count if batch_count > 0 else 0\n",
    "        loss_history[epoch] = avg_epoch_loss\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.6f}\")\n",
    "        \n",
    "        # Cleanup at epoch end\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7015/1942544810.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Loss: 0.397116\n",
      "\n",
      "Epoch 2/50\n",
      "------------------------------\n",
      "Epoch 2 Average Loss: 0.316766\n",
      "\n",
      "Epoch 3/50\n",
      "------------------------------\n",
      "Epoch 3 Average Loss: 0.262612\n",
      "\n",
      "Epoch 4/50\n",
      "------------------------------\n",
      "Epoch 4 Average Loss: 0.253129\n",
      "\n",
      "Epoch 5/50\n",
      "------------------------------\n",
      "Epoch 5 Average Loss: 0.252356\n",
      "\n",
      "Epoch 6/50\n",
      "------------------------------\n",
      "Epoch 6 Average Loss: 0.245461\n",
      "\n",
      "Epoch 7/50\n",
      "------------------------------\n",
      "Epoch 7 Average Loss: 0.236408\n",
      "\n",
      "Epoch 8/50\n",
      "------------------------------\n",
      "Epoch 8 Average Loss: 0.232772\n",
      "\n",
      "Epoch 9/50\n",
      "------------------------------\n",
      "Epoch 9 Average Loss: 0.229413\n",
      "\n",
      "Epoch 10/50\n",
      "------------------------------\n",
      "Epoch 10 Average Loss: 0.230995\n",
      "\n",
      "Epoch 11/50\n",
      "------------------------------\n",
      "Epoch 11 Average Loss: 0.231634\n",
      "\n",
      "Epoch 12/50\n",
      "------------------------------\n",
      "Epoch 12 Average Loss: 0.227808\n",
      "\n",
      "Epoch 13/50\n",
      "------------------------------\n",
      "Epoch 13 Average Loss: 0.229457\n",
      "\n",
      "Epoch 14/50\n",
      "------------------------------\n",
      "Epoch 14 Average Loss: 0.225681\n",
      "\n",
      "Epoch 15/50\n",
      "------------------------------\n",
      "Epoch 15 Average Loss: 0.227745\n",
      "\n",
      "Epoch 16/50\n",
      "------------------------------\n",
      "Epoch 16 Average Loss: 0.227382\n",
      "\n",
      "Epoch 17/50\n",
      "------------------------------\n",
      "Epoch 17 Average Loss: 0.223360\n",
      "\n",
      "Epoch 18/50\n",
      "------------------------------\n",
      "Epoch 18 Average Loss: 0.227049\n",
      "\n",
      "Epoch 19/50\n",
      "------------------------------\n",
      "Epoch 19 Average Loss: 0.221281\n",
      "\n",
      "Epoch 20/50\n",
      "------------------------------\n",
      "Epoch 20 Average Loss: 0.224349\n",
      "\n",
      "Epoch 21/50\n",
      "------------------------------\n",
      "Epoch 21 Average Loss: 0.222784\n",
      "\n",
      "Epoch 22/50\n",
      "------------------------------\n",
      "Epoch 22 Average Loss: 0.220645\n",
      "\n",
      "Epoch 23/50\n",
      "------------------------------\n",
      "Epoch 23 Average Loss: 0.215216\n",
      "\n",
      "Epoch 24/50\n",
      "------------------------------\n",
      "Epoch 24 Average Loss: 0.225557\n",
      "\n",
      "Epoch 25/50\n",
      "------------------------------\n",
      "Epoch 25 Average Loss: 0.223321\n",
      "\n",
      "Epoch 26/50\n",
      "------------------------------\n",
      "Epoch 26 Average Loss: 0.219392\n",
      "\n",
      "Epoch 27/50\n",
      "------------------------------\n",
      "Epoch 27 Average Loss: 0.220041\n",
      "\n",
      "Epoch 28/50\n",
      "------------------------------\n",
      "Epoch 28 Average Loss: 0.215311\n",
      "\n",
      "Epoch 29/50\n",
      "------------------------------\n",
      "Epoch 29 Average Loss: 0.212980\n",
      "\n",
      "Epoch 30/50\n",
      "------------------------------\n",
      "Epoch 30 Average Loss: 0.219097\n",
      "\n",
      "Epoch 31/50\n",
      "------------------------------\n",
      "Epoch 31 Average Loss: 0.216365\n",
      "\n",
      "Epoch 32/50\n",
      "------------------------------\n",
      "Epoch 32 Average Loss: 0.216268\n",
      "\n",
      "Epoch 33/50\n",
      "------------------------------\n",
      "Epoch 33 Average Loss: 0.222636\n",
      "\n",
      "Epoch 34/50\n",
      "------------------------------\n",
      "Epoch 34 Average Loss: 0.221670\n",
      "\n",
      "Epoch 35/50\n",
      "------------------------------\n",
      "Epoch 35 Average Loss: 0.218605\n",
      "\n",
      "Epoch 36/50\n",
      "------------------------------\n",
      "Epoch 36 Average Loss: 0.215008\n",
      "\n",
      "Epoch 37/50\n",
      "------------------------------\n",
      "Epoch 37 Average Loss: 0.214217\n",
      "\n",
      "Epoch 38/50\n",
      "------------------------------\n",
      "Epoch 38 Average Loss: 0.209474\n",
      "\n",
      "Epoch 39/50\n",
      "------------------------------\n",
      "Epoch 39 Average Loss: 0.210694\n",
      "\n",
      "Epoch 40/50\n",
      "------------------------------\n",
      "Epoch 40 Average Loss: 0.208834\n",
      "\n",
      "Epoch 41/50\n",
      "------------------------------\n",
      "Epoch 41 Average Loss: 0.204503\n",
      "\n",
      "Epoch 42/50\n",
      "------------------------------\n",
      "Epoch 42 Average Loss: 0.203085\n",
      "\n",
      "Epoch 43/50\n",
      "------------------------------\n",
      "Epoch 43 Average Loss: 0.201895\n",
      "\n",
      "Epoch 44/50\n",
      "------------------------------\n",
      "Epoch 44 Average Loss: 0.201570\n",
      "\n",
      "Epoch 45/50\n",
      "------------------------------\n",
      "Epoch 45 Average Loss: 0.204869\n",
      "\n",
      "Epoch 46/50\n",
      "------------------------------\n",
      "Epoch 46 Average Loss: 0.209990\n",
      "\n",
      "Epoch 47/50\n",
      "------------------------------\n",
      "Epoch 47 Average Loss: 0.207273\n",
      "\n",
      "Epoch 48/50\n",
      "------------------------------\n",
      "Epoch 48 Average Loss: 0.203415\n",
      "\n",
      "Epoch 49/50\n",
      "------------------------------\n",
      "Epoch 49 Average Loss: 0.200261\n",
      "\n",
      "Epoch 50/50\n",
      "------------------------------\n",
      "Epoch 50 Average Loss: 0.212967\n"
     ]
    }
   ],
   "source": [
    "loss_data = train_with_accumulation(epochs=epochs, training_data=training_data)#, validation_data= validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(extractor.state_dict(),'/home/faces2.0/models/extractor.pt')\n",
    "torch.save(topdown.state_dict(),'/home/faces2.0/models/topdown.pt')\n",
    "torch.save(classifier.state_dict(),'/home/faces2.0/models/classifier.pt')\n",
    "torch.save(bboxregression.state_dict(),'/home/faces2.0/models/bboxregression.pt')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
