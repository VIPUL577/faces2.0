{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from datasets import load_dataset\n",
    "from dataset_convert import FaceDetectionDataset as fd\n",
    "from dataset_convert import AnchorGenerator as ac\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"CUHK-CSE/wider_face\")\n",
    "\n",
    "# Convert to PyTorch format\n",
    "train_dataset = dataset['train'].with_format(\"torch\")\n",
    "val_dataset = dataset['validation'].with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'faces'],\n",
       "    num_rows: 12880\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = ac()\n",
    "tr = fd(train_dataset, ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FaceDetectionDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, widerface_dataset, anchor_generator):\n",
    "#         self.dataset = widerface_dataset\n",
    "#         self.copy =[]\n",
    "#         self.error = []\n",
    "#         self.anchor_generator = anchor_generator\n",
    "#         self.transforms = transforms.Compose([\n",
    "#             transforms.Resize((640, 640)),\n",
    "#             transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "#         ])\n",
    "#         self.all_anchors = self.anchor_generator.generate_anchors()\n",
    "#         for i in range(0,100):#len(widerface_dataset)):\n",
    "#             try:\n",
    "#                 self.redefine(i)\n",
    "#             except:\n",
    "#                 self.error.append(i)\n",
    "            \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "#     def __getitem__(self,idx):\n",
    "#         # return self.redefine(idx)\n",
    "#         return self.copy[idx]['image'],self.copy[idx]['targets']\n",
    "    \n",
    "#     def redefine(self, idx):\n",
    "#     # def __getitem__(self,idx):\n",
    "    \n",
    "#         data= self.dataset[idx]\n",
    "#         if data['faces']['bbox']!=[] :\n",
    "#             data['faces']['bbox'][:,0]=data['faces']['bbox'][:,0]*(640/1024)\n",
    "#             data['faces']['bbox'][:,1]=data['faces']['bbox'][:,1]*(640/data['image'].shape[1])\n",
    "#             data['faces']['bbox'][:,2]=data['faces']['bbox'][:,2]*(640/1024)\n",
    "#             data['faces']['bbox'][:,3]=data['faces']['bbox'][:,3]*(640/data['image'].shape[1])\n",
    "#             gt_boxes = data['faces']['bbox'].to(device)  # Shape: [num_faces, 4]\n",
    "#         else:\n",
    "#             gt_boxes = data['faces']['bbox']\n",
    "            \n",
    "#         image = self.transforms(data['image'].to(torch.float)/255)\n",
    "#         #feature_map_sizes)\n",
    "#         targets = []\n",
    "#         for level, anchors in enumerate(self.all_anchors):\n",
    "#             level_targets = self.assign_targets(gt_boxes, anchors)\n",
    "#             targets.append(level_targets)\n",
    "#         # return image,targets\n",
    "#         data['image'] = image\n",
    "#         # del image\n",
    "#         del data['faces']\n",
    "#         data['targets'] = targets\n",
    "#         self.copy.append(data)\n",
    "    \n",
    "    \n",
    "#     def assign_targets(self, gt_boxes, anchors, pos_threshold=0.5, neg_threshold=0.2):\n",
    "#         \"\"\"Assign ground truth boxes to anchors\"\"\"\n",
    "#         num_anchors = len(anchors)\n",
    "        \n",
    "#         if len(gt_boxes) == 0:\n",
    "#             # No faces in image\n",
    "#             return {\n",
    "#                 'cls_targets': torch.zeros(num_anchors, dtype=torch.long).to(device),\n",
    "#                 'bbox_targets': torch.zeros(num_anchors, 4).to(device),\n",
    "#                 'bbox_weights': torch.zeros(num_anchors).to(device)\n",
    "#             }\n",
    "        \n",
    "#         # Compute IoU between all anchors and ground truth boxes\n",
    "#         ious = self.compute_iou(anchors, gt_boxes)  # [num_anchors, num_gt]\n",
    "        \n",
    "#         # Find best matching ground truth for each anchor\n",
    "#         max_iou_per_anchor, max_iou_indices = ious.max(dim=1)\n",
    "        \n",
    "#         # Initialize targets\n",
    "#         cls_targets = torch.zeros(num_anchors, dtype=torch.long).to(device)  # 0: background\n",
    "#         bbox_targets = torch.zeros(num_anchors, 4).to(device)\n",
    "#         bbox_weights = torch.zeros(num_anchors).to(device)\n",
    "        \n",
    "#         # Positive samples (IoU > pos_threshold)\n",
    "#         positive_mask = max_iou_per_anchor > pos_threshold\n",
    "#         # print(positive_mask)\n",
    "#         cls_targets[positive_mask] = 1  # Face class\n",
    "#         bbox_weights[positive_mask] = 1.0\n",
    "        \n",
    "#         # Negative samples (IoU < neg_threshold)\n",
    "#         negative_mask = max_iou_per_anchor < neg_threshold\n",
    "#         cls_targets[negative_mask] = 0  # Background class\n",
    "        \n",
    "        \n",
    "#         # Encode bbox targets for positive samples\n",
    "#         if positive_mask.sum() > 0:\n",
    "#             positive_anchors = anchors[positive_mask]\n",
    "#             assigned_gt = gt_boxes[max_iou_indices[positive_mask]]\n",
    "#             bbox_targets[positive_mask] = self.encode_bbox_targets(assigned_gt, positive_anchors)\n",
    "\n",
    "#         return {\n",
    "#             'cls_targets': cls_targets,\n",
    "#             'bbox_targets': bbox_targets,\n",
    "#             'bbox_weights': bbox_weights\n",
    "#         }\n",
    "    \n",
    "#     def compute_iou(self, anchors, gt_boxes):\n",
    "#         \"\"\"Compute IoU between anchors and ground truth boxes\"\"\"\n",
    "#         # anchors: [num_anchors, 4] in [x1, y1, w, b] format\n",
    "#         # gt_boxes: [num_gt, 4] in [x1, y1, w, b] format\n",
    "        \n",
    "#         num_anchors = anchors.size(0)\n",
    "#         num_gt = gt_boxes.size(0)\n",
    "        \n",
    "#         # Expand dimensions for broadcasting\n",
    "#         anchors = anchors.unsqueeze(1).expand(num_anchors, num_gt, 4)\n",
    "#         gt_boxes = gt_boxes.unsqueeze(0).expand(num_anchors, num_gt, 4)\n",
    "        \n",
    "#         # Compute union\n",
    "#         anchor_area = anchors[:, :, 2] * anchors[:, :, 3]\n",
    "#         gt_area     = gt_boxes[:, :, 2] * gt_boxes[:, :, 3]\n",
    "\n",
    "#         # Compute intersection\n",
    "#         inter_x1 = torch.max(anchors[:, :, 0], gt_boxes[:, :, 0])   #x1   \n",
    "#         inter_y1 = torch.max(anchors[:, :, 1], gt_boxes[:, :, 1])   #y1\n",
    "#         inter_x2 = torch.min(anchors[:, :, 2]+anchors[:,:,0], gt_boxes[:, :, 2]+gt_boxes[:,:,0])   #x2\n",
    "#         inter_y2 = torch.min(anchors[:, :, 3]+anchors[:,:,1], gt_boxes[:, :, 3]+gt_boxes[:,:,1])   #y2\n",
    "        \n",
    "#         inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)\n",
    "#         union_area  = anchor_area + gt_area - inter_area\n",
    "\n",
    "#         # Compute IoU\n",
    "#         iou = inter_area / torch.clamp(union_area, min=1e-6)\n",
    "  \n",
    "#         return iou.to(device)\n",
    "    \n",
    "#     def encode_bbox_targets(self, gt_boxes, anchors):\n",
    "#         \"\"\"Encode ground truth boxes relative to anchors\"\"\"\n",
    "#         # Convert to center format\n",
    "#         anchor_widths = anchors[:, 2]\n",
    "#         anchor_heights = anchors[:, 3]\n",
    "#         anchor_cx = anchors[:, 0] + 0.5 * anchor_widths\n",
    "#         anchor_cy = anchors[:, 1] + 0.5 * anchor_heights\n",
    "        \n",
    "#         gt_widths = gt_boxes[:, 2]\n",
    "#         gt_heights = gt_boxes[:, 3]\n",
    "#         gt_cx = gt_boxes[:, 0] + 0.5 * gt_widths\n",
    "#         gt_cy = gt_boxes[:, 1] + 0.5 * gt_heights\n",
    "        \n",
    "#         # Encode as offsets\n",
    "#         target_dx = (gt_cx - anchor_cx) / anchor_widths\n",
    "#         target_dy = (gt_cy - anchor_cy) / anchor_heights\n",
    "#         target_dw = torch.log(gt_widths / anchor_widths)\n",
    "#         target_dh = torch.log(gt_heights / anchor_heights)\n",
    "#         x=torch.stack([target_dx, target_dy, target_dw, target_dh], dim=1).to(device)\n",
    "#         return x\n",
    "    \n",
    "# class AnchorGenerator:\n",
    "#     def __init__(self):\n",
    "#         # Define scales and aspect ratios for each FPN level\n",
    "#         self.scales = [128, 64, 32, 16]\n",
    "        \n",
    "#         self.aspect_ratios = [0.5, 1.0, 2.0]  # Common face aspect ratios\n",
    "#         self.anchor_scales = [2**0, 2**(1/3), 2**(2/3), 0.5]  # Sub-octave scales\n",
    "        \n",
    "#         # FPN level strides\n",
    "#         self.strides = [32, 16, 8, 4 ]  # Corresponding to your FPN levels\n",
    "        \n",
    "#     def generate_anchors(self, feature_map_sizes = [(20,20),(40,40),(80,80),(160,160)]):\n",
    "#         \"\"\"Generate anchors for all FPN levels\"\"\"\n",
    "#         all_anchors = []\n",
    "#         # feature_map_sizes = [(20,20),(40,40),(80,80),(160,160)] example\n",
    "#         for level, (h, w) in enumerate(feature_map_sizes):\n",
    "#             level_anchors = self.generate_level_anchors(\n",
    "#                 h, w, self.scales[level], self.strides[level]\n",
    "#             )\n",
    "#             all_anchors.append(level_anchors)\n",
    "#         return all_anchors\n",
    "    \n",
    "#     def generate_level_anchors(self, h, w, base_size, stride):\n",
    "#         \"\"\"Generate anchors for a single FPN level\"\"\"\n",
    "#         anchors = []\n",
    "        \n",
    "#         for i in range(h):\n",
    "#             for j in range(w):\n",
    "#                 cx = (j + 0.5) * stride\n",
    "#                 cy = (i + 0.5) * stride\n",
    "                \n",
    "#                 for aspect_ratio in self.aspect_ratios:\n",
    "#                     for scale in self.anchor_scales:\n",
    "#                         anchor_w = base_size * scale * math.sqrt(aspect_ratio)\n",
    "#                         anchor_h = base_size * scale / math.sqrt(aspect_ratio)\n",
    "\n",
    "#                         x1 = cx - anchor_w / 2\n",
    "#                         y1 = cy - anchor_h / 2\n",
    "#                         anchors.append([x1, y1,anchor_w ,anchor_h ])\n",
    "        \n",
    "#         return torch.tensor(anchors, dtype=torch.float32,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FaceDetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, widerface_dataset, anchor_generator):\n",
    "        self.dataset = widerface_dataset\n",
    "        self.anchor_generator = anchor_generator\n",
    "        self.all_anchors = self.anchor_generator.generate_anchors()\n",
    "\n",
    "        self.transform = nn.Sequential(\n",
    "            transforms.Resize((640, 640)),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "            ).to(device)\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            data= self.dataset[idx]\n",
    "            if data['faces']['bbox']!=[] :\n",
    "                data['faces']['bbox'][:,0]=data['faces']['bbox'][:,0]*(640/1024)\n",
    "                data['faces']['bbox'][:,1]=data['faces']['bbox'][:,1]*(640/data['image'].shape[1])\n",
    "                data['faces']['bbox'][:,2]=data['faces']['bbox'][:,2]*(640/1024)\n",
    "                data['faces']['bbox'][:,3]=data['faces']['bbox'][:,3]*(640/data['image'].shape[1])\n",
    "                gt_boxes = data['faces']['bbox'].to(device)  # Shape: [num_faces, 4]\n",
    "            else:\n",
    "                gt_boxes = []\n",
    "            image = self.transform(data['image'].to(torch.float)/255)\n",
    "            #feature_map_sizes)\n",
    "            targets = []\n",
    "            for level, anchors in enumerate(self.all_anchors):\n",
    "                level_targets = self.assign_targets(gt_boxes, anchors)\n",
    "                targets.append(level_targets)\n",
    "                \n",
    "            return image.to(device), targets\n",
    "        except Exception as e:\n",
    "            print(f\"Batch no. {idx}: {e}\")\n",
    "            return self.__getitem__(idx + 1)\n",
    "    \n",
    "    def assign_targets(self, gt_boxes, anchors, pos_threshold=0.5, neg_threshold=0.2):\n",
    "        \"\"\"Assign ground truth boxes to anchors\"\"\"\n",
    "        num_anchors = len(anchors)\n",
    "        \n",
    "        if len(gt_boxes) == 0:\n",
    "            # No faces in image\n",
    "            return {\n",
    "                'cls_targets': torch.zeros(num_anchors, dtype=torch.long).to(device),\n",
    "                'bbox_targets': torch.zeros(num_anchors, 4).to(device),\n",
    "                'bbox_weights': torch.zeros(num_anchors).to(device)\n",
    "            }\n",
    "        \n",
    "        # Compute IoU between all anchors and ground truth boxes\n",
    "        ious = self.compute_iou(anchors, gt_boxes)  # [num_anchors, num_gt]\n",
    "        \n",
    "        # Find best matching ground truth for each anchor\n",
    "        max_iou_per_anchor, max_iou_indices = ious.max(dim=1)\n",
    "        \n",
    "        # Initialize targets\n",
    "        cls_targets = torch.zeros(num_anchors, dtype=torch.long).to(device)  # 0: background\n",
    "        bbox_targets = torch.zeros(num_anchors, 4).to(device)\n",
    "        bbox_weights = torch.zeros(num_anchors).to(device)\n",
    "        \n",
    "        # Positive samples (IoU > pos_threshold)\n",
    "        positive_mask = max_iou_per_anchor > pos_threshold\n",
    "        # print(positive_mask)\n",
    "        cls_targets[positive_mask] = 1  # Face class\n",
    "        bbox_weights[positive_mask] = 1.0\n",
    "        \n",
    "        # Negative samples (IoU < neg_threshold)\n",
    "        negative_mask = max_iou_per_anchor < neg_threshold\n",
    "        cls_targets[negative_mask] = 0  # Background class\n",
    "        \n",
    "        \n",
    "        # Encode bbox targets for positive samples\n",
    "        if positive_mask.sum() > 0:\n",
    "            positive_anchors = anchors[positive_mask]\n",
    "            assigned_gt = gt_boxes[max_iou_indices[positive_mask]]\n",
    "            bbox_targets[positive_mask] = self.encode_bbox_targets(assigned_gt, positive_anchors)\n",
    "        return {\n",
    "            'cls_targets': cls_targets.reshape((cls_targets.shape[0],1)),\n",
    "            'bbox_targets': bbox_targets,\n",
    "            'bbox_weights': bbox_weights\n",
    "        }\n",
    "    \n",
    "    def compute_iou(self, anchors, gt_boxes):\n",
    "        \"\"\"Compute IoU between anchors and ground truth boxes\"\"\"\n",
    "        # anchors: [num_anchors, 4] in [x1, y1, w, b] format\n",
    "        # gt_boxes: [num_gt, 4] in [x1, y1, w, b] format\n",
    "        \n",
    "        num_anchors = anchors.size(0)\n",
    "        num_gt = gt_boxes.size(0)\n",
    "        \n",
    "        # Expand dimensions for broadcasting\n",
    "        anchors = anchors.unsqueeze(1).expand(num_anchors, num_gt, 4)\n",
    "        gt_boxes = gt_boxes.unsqueeze(0).expand(num_anchors, num_gt, 4)\n",
    "        \n",
    "        # Compute union\n",
    "        anchor_area = anchors[:, :, 2] * anchors[:, :, 3]\n",
    "        gt_area     = gt_boxes[:, :, 2] * gt_boxes[:, :, 3]\n",
    "\n",
    "        # Compute intersection\n",
    "        inter_x1 = torch.max(anchors[:, :, 0], gt_boxes[:, :, 0])   #x1   \n",
    "        inter_y1 = torch.max(anchors[:, :, 1], gt_boxes[:, :, 1])   #y1\n",
    "        inter_x2 = torch.min(anchors[:, :, 2]+anchors[:,:,0], gt_boxes[:, :, 2]+gt_boxes[:,:,0])   #x2\n",
    "        inter_y2 = torch.min(anchors[:, :, 3]+anchors[:,:,1], gt_boxes[:, :, 3]+gt_boxes[:,:,1])   #y2\n",
    "        \n",
    "        inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)\n",
    "        union_area  = anchor_area + gt_area - inter_area\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = inter_area / torch.clamp(union_area, min=1e-6)\n",
    "  \n",
    "        return iou.to(device)\n",
    "    \n",
    "    def encode_bbox_targets(self, gt_boxes, anchors):\n",
    "        \"\"\"Encode ground truth boxes relative to anchors\"\"\"\n",
    "        # Convert to center format\n",
    "        anchor_widths = anchors[:, 2]\n",
    "        anchor_heights = anchors[:, 3]\n",
    "        anchor_cx = anchors[:, 0] + 0.5 * anchor_widths\n",
    "        anchor_cy = anchors[:, 1] + 0.5 * anchor_heights\n",
    "        \n",
    "        gt_widths = gt_boxes[:, 2]\n",
    "        gt_heights = gt_boxes[:, 3]\n",
    "        gt_cx = gt_boxes[:, 0] + 0.5 * gt_widths\n",
    "        gt_cy = gt_boxes[:, 1] + 0.5 * gt_heights\n",
    "        \n",
    "        # Encode as offsets\n",
    "        target_dx = (gt_cx - anchor_cx) / anchor_widths\n",
    "        target_dy = (gt_cy - anchor_cy) / anchor_heights\n",
    "        target_dw = torch.log(gt_widths / anchor_widths)\n",
    "        target_dh = torch.log(gt_heights / anchor_heights)\n",
    "        x=torch.stack([target_dx, target_dy, target_dw, target_dh], dim=1).to(device)\n",
    "        return x\n",
    "    \n",
    "class AnchorGenerator:\n",
    "    def __init__(self):\n",
    "        # Define scales and aspect ratios for each FPN level\n",
    "        self.scales = [128, 64, 32, 16]\n",
    "        \n",
    "        self.aspect_ratios = [0.5, 1.0, 2.0]  # Common face aspect ratios\n",
    "        self.anchor_scales = [2**0, 2**(1/3), 2**(2/3), 0.5]  # Sub-octave scales\n",
    "        \n",
    "        # FPN level strides\n",
    "        self.strides = [32, 16, 8, 4 ]  # Corresponding to your FPN levels\n",
    "        \n",
    "    def generate_anchors(self, feature_map_sizes = [(20,20),(40,40),(80,80),(160,160)]):\n",
    "        \"\"\"Generate anchors for all FPN levels\"\"\"\n",
    "        all_anchors = []\n",
    "        # feature_map_sizes = [(20,20),(40,40),(80,80),(160,160)] example\n",
    "        for level, (h, w) in enumerate(feature_map_sizes):\n",
    "            level_anchors = self.generate_level_anchors(\n",
    "                h, w, self.scales[level], self.strides[level]\n",
    "            )\n",
    "            all_anchors.append(level_anchors)\n",
    "            \n",
    "        return all_anchors\n",
    "    \n",
    "    def generate_level_anchors(self, h, w, base_size, stride):\n",
    "        \"\"\"Generate anchors for a single FPN level\"\"\"\n",
    "        anchors = []\n",
    "        \n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                cx = (j + 0.5) * stride\n",
    "                cy = (i + 0.5) * stride\n",
    "                \n",
    "                for aspect_ratio in self.aspect_ratios:\n",
    "                    for scale in self.anchor_scales:\n",
    "                        anchor_w = base_size * scale * math.sqrt(aspect_ratio)\n",
    "                        anchor_h = base_size * scale / math.sqrt(aspect_ratio)\n",
    "\n",
    "                        x1 = cx - anchor_w / 2\n",
    "                        y1 = cy - anchor_h / 2\n",
    "                        anchors.append([x1, y1,anchor_w ,anchor_h ])\n",
    "        \n",
    "        return torch.tensor(anchors, dtype=torch.float32,device=device)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12880\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     train_data[i]\n",
      "Cell \u001b[0;32mIn[159], line 29\u001b[0m, in \u001b[0;36mFaceDetectionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level, anchors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_anchors):\n\u001b[0;32m---> 29\u001b[0m     level_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_targets(gt_boxes, anchors)\n\u001b[1;32m     30\u001b[0m     targets\u001b[38;5;241m.\u001b[39mappend(level_targets)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\u001b[38;5;241m.\u001b[39mto(device), targets\n",
      "Cell \u001b[0;32mIn[159], line 50\u001b[0m, in \u001b[0;36mFaceDetectionDataset.assign_targets\u001b[0;34m(self, gt_boxes, anchors, pos_threshold, neg_threshold)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_targets\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mzeros(num_anchors, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox_targets\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mzeros(num_anchors, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox_weights\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mzeros(num_anchors)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     47\u001b[0m     }\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Compute IoU between all anchors and ground truth boxes\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m ious \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_iou(anchors, gt_boxes)  \u001b[38;5;66;03m# [num_anchors, num_gt]\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Find best matching ground truth for each anchor\u001b[39;00m\n\u001b[1;32m     53\u001b[0m max_iou_per_anchor, max_iou_indices \u001b[38;5;241m=\u001b[39m ious\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[159], line 108\u001b[0m, in \u001b[0;36mFaceDetectionDataset.compute_iou\u001b[0;34m(self, anchors, gt_boxes)\u001b[0m\n\u001b[1;32m    105\u001b[0m union_area  \u001b[38;5;241m=\u001b[39m anchor_area \u001b[38;5;241m+\u001b[39m gt_area \u001b[38;5;241m-\u001b[39m inter_area\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Compute IoU\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m iou \u001b[38;5;241m=\u001b[39m inter_area \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(union_area, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iou\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(12880):\n",
    "    train_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = AnchorGenerator()\n",
    "train_data = FaceDetectionDataset(train_dataset,anchors)\n",
    "val_data = FaceDetectionDataset(val_dataset,anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "                                            ^     ^^^^ ^^^ ^ ^ ^^^ ^^ ^ ^^ ^^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "^^^^^^^^^^\n",
      "^^^  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^    self = reduction.pickle.load(from_parent)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "     self = reduction.pickle.load(from_parent)AttributeError \n",
      " :  Can't get attribute 'FaceDetectionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>  \n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^ ^ ^ ^ ^ ^ ^  ^  ^  ^  ^  \n",
      "     ^ ^^^^^^^^^^^^^^^^AttributeError^: ^Can't get attribute 'FaceDetectionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "AttributeErrorAttributeError: : Can't get attribute 'FaceDetectionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Can't get attribute 'FaceDetectionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 18175, 18176, 18177, 18178) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m wait([\u001b[38;5;28mself\u001b[39m], timeout)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:1136\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 18175) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      2\u001b[0m     train_data,\n\u001b[1;32m      3\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,           \u001b[38;5;66;03m# Reduce if GPU memory is full\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Test dataset directly\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Testing if the loop is running successfully \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,(image,bbox) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m((dataloader)):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m         image\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 18175, 18176, 18177, 18178) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=8,           # Reduce if GPU memory is full\n",
    "    num_workers=4,           # Use 4-8 workers for CPU-bound tasks\n",
    "    # pin_memory=True,         # Faster CPU->GPU transfer\n",
    "    persistent_workers=True, # Reuse workers across epochs\n",
    "    prefetch_factor=4,       # Prefetch more batches\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Test dataset directly\n",
    "# Testing if the loop is running successfully \n",
    "for i,(image,bbox) in enumerate((dataloader)):\n",
    "    try:\n",
    "        image\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)    \n",
      "exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "                                 ^ ^ ^ ^ ^          ^  ^ ^^^ ^ ^ ^      ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^^^^^\n",
      "^^^^^^^  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "^  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "\n",
      "\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           self = reduction.pickle.load(from_parent)\n",
      "    ^^^^^^^^^^^^^^^^^^ ^    ^self = reduction.pickle.load(from_parent)^\n",
      "       ^ ^ ^ ^ self = reduction.pickle.load(from_parent)  \n",
      " ^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^AttributeError^^ ^ ^ ^: ^^Can't get attribute 'FaceDetectionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>^^\n",
      "     ^ ^ ^ ^ ^ ^   ^^^^^^^^^ ^^^^^^^^^\n",
      "^^^ ^^ ^AttributeError :  Can't get attribute 'FaceDetectionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)> \n",
      "^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^^^AttributeError^^: ^Can't get attribute 'FaceDetectionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>^\n",
      "^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'FaceDetectionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 17798, 17799, 17800, 17801) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m wait([\u001b[38;5;28mself\u001b[39m], timeout)\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/multiprocessing/connection.py:1136\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     ready \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 17799) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test dataset directly\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,(image,bbox) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m((dataloader)):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m         image\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 17798, 17799, 17800, 17801) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Test dataset directly\n",
    "for i,(image,bbox) in enumerate((dataloader)):\n",
    "    try:\n",
    "        image\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =[]\n",
    "for i in range (450,500):\n",
    "    \n",
    "    try:\n",
    "       a.append(train_data[i])\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m140\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     a[i]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range (0,140):\n",
    "\n",
    "    a[i]\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[80][0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6999"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 744\n",
    "# 1159\n",
    "# 3125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1968, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[3125\n",
    "]['faces']['bbox'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data[\u001b[38;5;241m3125\u001b[39m]\n",
      "Cell \u001b[0;32mIn[82], line 28\u001b[0m, in \u001b[0;36mFaceDetectionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level, anchors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_anchors):\n\u001b[0;32m---> 28\u001b[0m     level_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_targets(gt_boxes, anchors)\n\u001b[1;32m     29\u001b[0m     targets\u001b[38;5;241m.\u001b[39mappend(level_targets)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\u001b[38;5;241m.\u001b[39mto(device), targets\n",
      "Cell \u001b[0;32mIn[82], line 48\u001b[0m, in \u001b[0;36mFaceDetectionDataset.assign_targets\u001b[0;34m(self, gt_boxes, anchors, pos_threshold, neg_threshold)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_targets\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mzeros(num_anchors, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox_targets\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mzeros(num_anchors, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox_weights\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mzeros(num_anchors)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m     }\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Compute IoU between all anchors and ground truth boxes\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m ious \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_iou(anchors, gt_boxes)  \u001b[38;5;66;03m# [num_anchors, num_gt]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Find best matching ground truth for each anchor\u001b[39;00m\n\u001b[1;32m     51\u001b[0m max_iou_per_anchor, max_iou_indices \u001b[38;5;241m=\u001b[39m ious\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[82], line 99\u001b[0m, in \u001b[0;36mFaceDetectionDataset.compute_iou\u001b[0;34m(self, anchors, gt_boxes)\u001b[0m\n\u001b[1;32m     97\u001b[0m inter_x1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(anchors[:, :, \u001b[38;5;241m0\u001b[39m], gt_boxes[:, :, \u001b[38;5;241m0\u001b[39m])   \u001b[38;5;66;03m#x1   \u001b[39;00m\n\u001b[1;32m     98\u001b[0m inter_y1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(anchors[:, :, \u001b[38;5;241m1\u001b[39m], gt_boxes[:, :, \u001b[38;5;241m1\u001b[39m])   \u001b[38;5;66;03m#y1\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m inter_x2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(anchors[:, :, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m+\u001b[39manchors[:,:,\u001b[38;5;241m0\u001b[39m], gt_boxes[:, :, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m+\u001b[39mgt_boxes[:,:,\u001b[38;5;241m0\u001b[39m])   \u001b[38;5;66;03m#x2\u001b[39;00m\n\u001b[1;32m    100\u001b[0m inter_y2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(anchors[:, :, \u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m+\u001b[39manchors[:,:,\u001b[38;5;241m1\u001b[39m], gt_boxes[:, :, \u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m+\u001b[39mgt_boxes[:,:,\u001b[38;5;241m1\u001b[39m])   \u001b[38;5;66;03m#y2\u001b[39;00m\n\u001b[1;32m    102\u001b[0m inter_area \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(inter_x2 \u001b[38;5;241m-\u001b[39m inter_x1, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(inter_y2 \u001b[38;5;241m-\u001b[39m inter_y1, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data[3125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((10,))\n",
    "x.requires_grad = True\n",
    "err = torch.rand((10,))\n",
    "err.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6191, 0.5483, 0.2718, 0.0808, 0.5460, 0.6978, 0.3387, 0.8937, 0.2608,\n",
       "        0.7522], requires_grad=True)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8742, 0.8747, 0.4577, 0.7391, 0.2139, 0.8044, 0.3950, 0.7807, 0.1670,\n",
       "        0.6212], requires_grad=True)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
